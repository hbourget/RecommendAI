{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __CPE__ 2022-23\n",
    "\n",
    "## Text Mining \n",
    "\n",
    "<font size=\"5\"> Analyse de Polarités  des avis sur des achats +  Classification de phrases</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __Objectifs__ : apprendre par mimétisme à partir d'exemples\n",
    "- Appliquer de multiples techniques/méthodes et donner un tableau comparatif "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Traitements effectués (et les scores) en vue de \"choisir\" le meilleur modèle**\n",
    "__Les scores d'une précédente exécution__  <font color=\"red\"> (changement possible)</font>\n",
    "* Préparation : Codage Tf (Term frequency) puis TfIdf (Term frequency Inverse Document Frequency)\n",
    "* Méthode AD : Un seul Arbre de Décision : score 0.71\n",
    "* Méthode RF : Random Forest (50) : score 0.77\n",
    "* Evaluation : ROC de RF (AUC = 0.84)\n",
    "* Méthode : Logit : score 0.80\n",
    "* Méthode : MNB : score 0.81 (MultiNomialBayes)\n",
    "* Méthode : RF (avec différents depths) et tjs 50 trees\n",
    "    - clf10 : score = 0.74 (Profondeur 10, \"clf\" = classifieur)\n",
    "    - clf30 : score = 0.785\n",
    "    - clf50 : score = 0.796\n",
    "    - clf70 : score = 0.785\n",
    "    - clf90 : score = 0.789\n",
    "\n",
    "* Méthode : RF avec 200 arbres / depth 50 : score = 0.788\n",
    "* Méthode : GBoost : score = 0.78\n",
    "\n",
    "<font size=\"4\">On décide de choisir Logit pour la suite (on pourrait prendre MNB, voir ci-après)</font>\n",
    "* Préparation : n-grams (démo n=3 : 3-grams)\n",
    "* Préparation : n-gramm (n=2) sur lesquels on fait TfIdf puis\n",
    "* Méthode : Logit : score = 0.8\n",
    "* Test / Validation : même chose (Logit) avec XV : score = 0.84 <--- Best2\n",
    "* Préparation : n-gramm (n=2) sur lesquels on fait TfIdf puis\n",
    "* Méthode : MNB avec Test / Validation XV (avec 20 folds) : score = 0.853 <--- Best1\n",
    "\n",
    "<font size=\"4\">On délaisse les n-grams (on revient au mono-gram)</font>\n",
    "* Préparation : La représentation Tf puis réduction à 50 features par SVD puis\n",
    "* Méthode : RF : score = 0.68\n",
    "* Méthode : Logit (sur ce rézsultat SVD) : 0.75\n",
    "* Préparation : On reprend TfIdf puis SVD à 300 features\n",
    "* Méthode : Logit (sur TfIdf x SVD à 300) : score = 0.78\n",
    "\n",
    "<font size=\"4\">Word2Vect</font>\n",
    "* Préparation : Word2Vect sur la BD d'origine (Word2vect avec vecteur de mot de taille 300, taille window=20) !\n",
    "* Préparation : Doc2Vect manuelle (somme des vecteurs Word2Vect)\n",
    "* Méthode : Logit sur ce Doc2Vect : score = 0.56\n",
    "\n",
    "<font size=\"4\">Reste à faire (délicat) Word2Vect sur données pré-entrainées Google avec spacy </br>\n",
    "Lire ci-dessous \"A propose des classifieurs en Text Mining appliqués\"</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Savoir où on est (sur nos machines)\n",
    "\n",
    "Pour connaître la version du Python, le répertoire actuel et vérifier si les fichiers s'y trouvent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9.13 (main, Aug 25 2022, 23:26:10) \n",
      "[GCC 11.2.0]\n",
      "/home/alex/TextMining-Big-Data/Big-Data-22-23/100-BE-22-23-ECL-From-21-22/Codes-22-23/POUR-CPE-DM-22-23\n",
      "['DF_Papers.csv', 'sentiment_labelled_sentences.zip', '1-OK-Polarite-sentiments-2023-Sujet-principal.ipynb', 'sentiment labelled sentences', '.ipynb_checkpoints', '__MACOSX', 'Untitled.ipynb']\n",
      "/home/alex/TextMining-Big-Data/Big-Data-22-23/100-BE-22-23-ECL-From-21-22/Codes-22-23/POUR-CPE-DM-22-23\n",
      " 1-OK-Polarite-sentiments-2023-Sujet-principal.ipynb\n",
      " DF_Papers.csv\n",
      " __MACOSX\n",
      "'sentiment labelled sentences'\n",
      " sentiment_labelled_sentences.zip\n",
      " Untitled.ipynb\n"
     ]
    }
   ],
   "source": [
    "import os,sys\n",
    "\n",
    "# os.chdir(os.path.dirname(__file__)) # pour se placer dans le répertoire de \"ce\" fichier ipynb\n",
    "print(sys.version)\n",
    "print(os.getcwd())\n",
    "print(os.listdir())\n",
    "\n",
    "# On peut aussi bien écrire :\n",
    "!pwd  # ou %pwd\n",
    "!ls   # ou %ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### La différence entre '!' et '%' :\n",
    "- %    s'adresse au noyau ipykernel      \n",
    "- !    s'adresse au shell (Linux)   \n",
    "\n",
    "D'une manière générale, une commande avec '!' s'exécute (via un processus) et on revient à l'état précédent\n",
    "tandis qu'avec '%', les effets (par exemple ce placer dans un répertoire) sont permanents.       \n",
    "Pour simplifier, les deux sont identiques (sauf dans quelques cas particuliers). \n",
    "\n",
    "**Exemples :**\n",
    "- !cd toto    \n",
    "nous place dans le répertoite 'toto' mais ensuite, on retourn au répertoire d'avant cette commande tandis que   \n",
    "- %cd toto    \n",
    "nous place dans \"toto\" et on y reste !\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\"> Revenons à notre sujet : Classification de phrases </font>\n",
    "En fouille de données textuelles, plus le vocabulaire est étendu, plus il faut des données !\n",
    "\n",
    "Le problème traité ici est (assez) classique en Text-Mining, on cherche à catégoriser des phrases en **Polarite positif ou négatif**. Ce pourrait être aussi classer des spams, résumer un texte, .....\n",
    "\n",
    "Le problème est ici simplifié : une donnée = <une phrase, un label>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1- Les données\n",
    "\n",
    "Elles proviennent du data set \"Polarite Labelled Sentences Data Set\".\n",
    "\n",
    "Il faut installer (une seule fois), pour avoir ces données (et plein d'autres)\n",
    "* !pip3 install papierstat.datasets\n",
    "* !pip3 install papierstat\n",
    "* !pip3 install dbfread\n",
    "* !pip3 install geopandas\n",
    "* !pip3 install pyensae\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 - Chargement des données\n",
    "\n",
    "**Deux façons de faire (choisir l'une des deux) :** \n",
    "\n",
    "### 1.1.1) Chargement de la BD   __Soit directement par__  pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False : # JE l'ai déjà fait ! Pour refaire, remplacer \"False\" par \"True\"\n",
    "    !pip install papierstat\n",
    "    !pip install pyquickhelper\n",
    "    !python3 -m pip install --upgrade pip\n",
    "    !pip install dbfread\n",
    "    !pip install pyensae\n",
    "    !pip install chardet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>So there is no way for me to plug it in here i...</td>\n",
       "      <td>0</td>\n",
       "      <td>amazon_cells_labelled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good case, Excellent value.</td>\n",
       "      <td>1</td>\n",
       "      <td>amazon_cells_labelled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Great for the jawbone.</td>\n",
       "      <td>1</td>\n",
       "      <td>amazon_cells_labelled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tied to charger for conversations lasting more...</td>\n",
       "      <td>0</td>\n",
       "      <td>amazon_cells_labelled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The mic is great.</td>\n",
       "      <td>1</td>\n",
       "      <td>amazon_cells_labelled</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  sentiment  \\\n",
       "0  So there is no way for me to plug it in here i...          0   \n",
       "1                        Good case, Excellent value.          1   \n",
       "2                             Great for the jawbone.          1   \n",
       "3  Tied to charger for conversations lasting more...          0   \n",
       "4                                  The mic is great.          1   \n",
       "\n",
       "                  source  \n",
       "0  amazon_cells_labelled  \n",
       "1  amazon_cells_labelled  \n",
       "2  amazon_cells_labelled  \n",
       "3  amazon_cells_labelled  \n",
       "4  amazon_cells_labelled  "
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from papierstat.datasets import load_sentiment_dataset\n",
    "df_papers = load_sentiment_dataset()\n",
    "df_papers.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2) Chargement de la BD : __Soit en chargeant le fichier csv fourni__.\n",
    "\n",
    "__Remarque__ : ces données peuvent être téléchargées  (à demeure) : le fichier de données  \"amazon_cells_labelled.txt\" est égalment disponible et on peut charger (directement) l'archive sur  https://archive.ics.uci.edu/ml/machine-learning-databases/00331/\n",
    "* Le fichier  \"amazon_cells_labelled.txt\" est ef fait un fichier \".tsv\" : les classes données après une tabulation.\n",
    "- <ins> Notons que ce dernier n'a pas la colonne `source` (inutile pour nous ici)</ins>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>So there is no way for me to plug it in here i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good case, Excellent value.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Great for the jawbone.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tied to charger for conversations lasting more...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The mic is great.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  sentiment\n",
       "0  So there is no way for me to plug it in here i...          0\n",
       "1                        Good case, Excellent value.          1\n",
       "2                             Great for the jawbone.          1\n",
       "3  Tied to charger for conversations lasting more...          0\n",
       "4                                  The mic is great.          1"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "csv_txt='./DF_Papers.csv'\n",
    "df_papers = pd.read_csv(csv_txt, header=0) \n",
    "df_papers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2- Préparation des données     \n",
    "### 1.2.1) Conserver les colonnes utiles    \n",
    "on enlève la 3e colonne (source) du data frame (suivant la méthode de chargement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Deux manières d'enlever la colonne \"source\" (si présente) :__\n",
    "1) __soit en tronquant directement le data frame__     \n",
    "Ici, pas d'erreur si la 3e col n'est pas présente.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__La 1ère solution :__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>So there is no way for me to plug it in here i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good case, Excellent value.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Great for the jawbone.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tied to charger for conversations lasting more...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The mic is great.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2995</th>\n",
       "      <td>I think food should have flavor and texture an...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2996</th>\n",
       "      <td>Appetite instantly gone.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2997</th>\n",
       "      <td>Overall I was not impressed and would not go b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2998</th>\n",
       "      <td>The whole experience was underwhelming, and I ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999</th>\n",
       "      <td>Then, as if I hadn't wasted enough of my life ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence  sentiment\n",
       "0     So there is no way for me to plug it in here i...          0\n",
       "1                           Good case, Excellent value.          1\n",
       "2                                Great for the jawbone.          1\n",
       "3     Tied to charger for conversations lasting more...          0\n",
       "4                                     The mic is great.          1\n",
       "...                                                 ...        ...\n",
       "2995  I think food should have flavor and texture an...          0\n",
       "2996                           Appetite instantly gone.          0\n",
       "2997  Overall I was not impressed and would not go b...          0\n",
       "2998  The whole experience was underwhelming, and I ...          0\n",
       "2999  Then, as if I hadn't wasted enough of my life ...          0\n",
       "\n",
       "[3000 rows x 2 columns]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_papers=df_papers[['sentence', 'sentiment']] # ON conserve 2 cols.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) .__Soit par la fonction _drop_ de Pandas__     \n",
    "Evidemment, si vous avez appliqué la cellule précédente, vous aurez une erreur ici !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__La 2e manière__ : erreur possible si colonne absente (d'où le try-except)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "try :\n",
    "    df_papers=df_papers.drop(['source'], axis=1) # axis=1 : concerne les colonnes\n",
    "except : pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vérifions que nous avon sbien 2 colonnes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 2)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_papers.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2) Renommage  des colonnes (attributs)   \n",
    "Pour plus de clarté, on **renomme** les colonnes des données en 'Avis' et 'Polarite'\n",
    "* __Avis__ est l'avis exprimé\n",
    "* __Polarite__ est l'opinion (sentiment) (0 ou 1 : négatif ou positif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Avis</th>\n",
       "      <th>Polarite</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>So there is no way for me to plug it in here i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good case, Excellent value.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Great for the jawbone.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tied to charger for conversations lasting more...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The mic is great.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2995</th>\n",
       "      <td>I think food should have flavor and texture an...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2996</th>\n",
       "      <td>Appetite instantly gone.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2997</th>\n",
       "      <td>Overall I was not impressed and would not go b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2998</th>\n",
       "      <td>The whole experience was underwhelming, and I ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999</th>\n",
       "      <td>Then, as if I hadn't wasted enough of my life ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Avis  Polarite\n",
       "0     So there is no way for me to plug it in here i...         0\n",
       "1                           Good case, Excellent value.         1\n",
       "2                                Great for the jawbone.         1\n",
       "3     Tied to charger for conversations lasting more...         0\n",
       "4                                     The mic is great.         1\n",
       "...                                                 ...       ...\n",
       "2995  I think food should have flavor and texture an...         0\n",
       "2996                           Appetite instantly gone.         0\n",
       "2997  Overall I was not impressed and would not go b...         0\n",
       "2998  The whole experience was underwhelming, and I ...         0\n",
       "2999  Then, as if I hadn't wasted enough of my life ...         0\n",
       "\n",
       "[3000 rows x 2 columns]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_papers=df_papers.rename(columns = {'sentence': 'Avis', 'sentiment' : 'Polarite'}) \n",
    "df_papers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Info :__ Les attributs (les colonnes de la matrice) sous 2 formes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Avis', 'Polarite'], dtype=object)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Les attributs (colonnes)\n",
    "df_papers.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Avis', 'Polarite'], dtype='object')"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_papers.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Info : Combien de chaque polarité__ ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Avis</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Polarite</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Avis\n",
       "Polarite      \n",
       "0         1500\n",
       "1         1500"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Si on veut un groupement inutile (car les Avis ne sont pas répétés)!\n",
    "#df_papers.groupby(['Avis', 'Polarite']).count() \n",
    "\n",
    "# Le bon groupement\n",
    "df_papers.groupby(['Polarite']).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2- Vers l'analyse des \"avis\" (*sentiments*)\n",
    "## 2.1- Répartition des données pour l'analyse\n",
    "On les divise en 2 ensembles d'__apprentissage__ et de __test__\n",
    "\n",
    "**A propos du découpage en \"train\" et \"test\" ?**\n",
    "\n",
    "Voir https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "\n",
    "Et https://scikit-learn.org/stable/modules/cross_validation.html#stratification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__On découpe en train and test : par défaut 75% et 25%__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On découpe en train and test : par défaut 75% et 25%\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_papers[[\"Avis\"]], df_papers['Polarite'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Pour changer les proportions par défaut et avoir par exemple 80% 20%__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pour changer les proportions par défaut et avoir par exemple 80% 20%\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(df_papers[[\"Avis\"]], df_papers['Polarite'], train_size=0.80, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Combien dans chaque paquet train / test ?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2400, 1)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape  # les dimensions : (nb_lignes, nb_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600, 1)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Mining : l'approche classique (vector space)\n",
    "   Rappel cours : on cherche à transformer le corpus en une matrice numérique pour y appliquer ensuite les méthodes \"classiques\" du Data Mining.\n",
    "   \n",
    "   Cette matrice peut contenir des valeurs (réelles) calculées de différentes façons; elles représenteront les \"termes\" (mots) dans le corpus. Une de ces façons est de calculer un indice `TfIdf` pour chaque mot du corpus.\n",
    "    \n",
    "<font color=\"red\"> TfIdf </font> est une méthode de convertion ses __mots__  en attributs __numériques__ (*features du vector space*).\n",
    "* `Tf(terme)` : *term fréquency* (pour un mot/terme) = la fréquence d'un terme dans le document\n",
    "* `Idf(terme)` : *Inverse document Frequency* (pour un mot/terme) = log(nombre total de documents __divisé__ par le nombre de documents où ce terme  apparaît;\n",
    "* `TfIdf(terme)` : $TF(terme) \\times Idf(terme)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2- Calcul de la matrice numérique "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1- Un exemple simple de calcul de la matrice numérique (avec des données triviales pour tester)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "* * *\n",
    "**D'abord, un exemple simple de comptage des occurrences**   \n",
    "On se donne un texte simple :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is the first document.',\n",
       " 'This document seems to be the second document.',\n",
       " 'And this is the third one.',\n",
       " 'Is this the first fourth document or the third one from the second one !?']"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer # nécessaire !\n",
    "import pandas as pd \n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document seems to be the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first fourth document or the third one from the second one !?',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Puis compter les nb occurrences**    \n",
    "<u>Remarque :</u>     \n",
    "* _get_feature_names()_    \n",
    "OU (si erreur dûe à la version)     \n",
    "* _get_feature_names_out()_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and' 'be' 'document' 'first' 'fourth' 'from' 'is' 'one' 'or' 'second'\n",
      " 'seems' 'the' 'third' 'this' 'to']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>be</th>\n",
       "      <th>document</th>\n",
       "      <th>first</th>\n",
       "      <th>fourth</th>\n",
       "      <th>from</th>\n",
       "      <th>is</th>\n",
       "      <th>one</th>\n",
       "      <th>or</th>\n",
       "      <th>second</th>\n",
       "      <th>seems</th>\n",
       "      <th>the</th>\n",
       "      <th>third</th>\n",
       "      <th>this</th>\n",
       "      <th>to</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   and  be  document  first  fourth  from  is  one  or  second  seems  the  \\\n",
       "0    0   0         1      1       0     0   1    0   0       0      0    1   \n",
       "1    0   1         2      0       0     0   0    0   0       1      1    1   \n",
       "2    1   0         0      0       0     0   1    1   0       0      0    1   \n",
       "3    0   0         1      1       1     1   1    2   1       1      0    3   \n",
       "\n",
       "   third  this  to  \n",
       "0      0     1   0  \n",
       "1      0     1   1  \n",
       "2      1     1   0  \n",
       "3      1     1   0  "
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer() #n comptage du nbr d'occ\n",
    "matrice_occurrences = vectorizer.fit_transform(corpus) # cette matrice est du type \"scipy sparse matrix\"\n",
    "print(vectorizer.get_feature_names_out()) #  get_feature_names, get_feature_names_out pour Python 3.9 (!)\n",
    "matrice = pd.DataFrame(matrice_occurrences.toarray(),columns=vectorizer.get_feature_names_out())\n",
    "matrice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**On a donc notre matrice des occurrences**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1.1- Matrice des n-grams (n=1...)     \n",
    "La matrice ci-dessus était la matrice des mono-grammes (mot seuls).     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Si on veut la même chose pour les <ins>bi-grams</ins> (voir ci-dessous)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and this' 'be the' 'document or' 'document seems' 'first document'\n",
      " 'first fourth' 'fourth document' 'from the' 'is the' 'is this' 'one from'\n",
      " 'or the' 'second document' 'second one' 'seems to' 'the first'\n",
      " 'the second' 'the third' 'third one' 'this document' 'this is' 'this the'\n",
      " 'to be']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and this</th>\n",
       "      <th>be the</th>\n",
       "      <th>document or</th>\n",
       "      <th>document seems</th>\n",
       "      <th>first document</th>\n",
       "      <th>first fourth</th>\n",
       "      <th>fourth document</th>\n",
       "      <th>from the</th>\n",
       "      <th>is the</th>\n",
       "      <th>is this</th>\n",
       "      <th>...</th>\n",
       "      <th>second one</th>\n",
       "      <th>seems to</th>\n",
       "      <th>the first</th>\n",
       "      <th>the second</th>\n",
       "      <th>the third</th>\n",
       "      <th>third one</th>\n",
       "      <th>this document</th>\n",
       "      <th>this is</th>\n",
       "      <th>this the</th>\n",
       "      <th>to be</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   and this  be the  document or  document seems  first document  \\\n",
       "0         0       0            0               0               1   \n",
       "1         0       1            0               1               0   \n",
       "2         1       0            0               0               0   \n",
       "3         0       0            1               0               0   \n",
       "\n",
       "   first fourth  fourth document  from the  is the  is this  ...  second one  \\\n",
       "0             0                0         0       1        0  ...           0   \n",
       "1             0                0         0       0        0  ...           0   \n",
       "2             0                0         0       1        0  ...           0   \n",
       "3             1                1         1       0        1  ...           1   \n",
       "\n",
       "   seems to  the first  the second  the third  third one  this document  \\\n",
       "0         0          1           0          0          0              0   \n",
       "1         1          0           1          0          0              1   \n",
       "2         0          0           0          1          1              0   \n",
       "3         0          1           1          1          1              0   \n",
       "\n",
       "   this is  this the  to be  \n",
       "0        1         0      0  \n",
       "1        0         0      1  \n",
       "2        1         0      0  \n",
       "3        0         1      0  \n",
       "\n",
       "[4 rows x 23 columns]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
    "matrice_occurrences = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names_out())#  get_feature_names, get_feature_names_out pour Python 3.9\n",
    "matrice = pd.DataFrame(matrice_occurrences.toarray(), columns=vectorizer.get_feature_names_out())#  get_feature_names, get_feature_names_out pour Python 3.9\n",
    "\n",
    "matrice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1.2- Le vocabulaire (bi-grammes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this is': 20,\n",
       " 'is the': 8,\n",
       " 'the first': 15,\n",
       " 'first document': 4,\n",
       " 'this document': 19,\n",
       " 'document seems': 3,\n",
       " 'seems to': 14,\n",
       " 'to be': 22,\n",
       " 'be the': 1,\n",
       " 'the second': 16,\n",
       " 'second document': 12,\n",
       " 'and this': 0,\n",
       " 'the third': 17,\n",
       " 'third one': 18,\n",
       " 'is this': 9,\n",
       " 'this the': 21,\n",
       " 'first fourth': 5,\n",
       " 'fourth document': 6,\n",
       " 'document or': 2,\n",
       " 'or the': 11,\n",
       " 'one from': 10,\n",
       " 'from the': 7,\n",
       " 'second one': 13}"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Les valeurs seront les rangs (les indices)\n",
    "vectorizer.vocabulary_  # Le vocabulaire avec les indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('this is', 20), ('is the', 8), ('the first', 15), ('first document', 4), ('this document', 19), ('document seems', 3), ('seems to', 14), ('to be', 22), ('be the', 1), ('the second', 16), ('second document', 12), ('and this', 0), ('the third', 17), ('third one', 18), ('is this', 9), ('this the', 21), ('first fourth', 5), ('fourth document', 6), ('document or', 2), ('or the', 11), ('one from', 10), ('from the', 7), ('second one', 13)])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# La même chose sous forme d'une liste de tuples (utilisé dans une itération p. ex.)\n",
    "vectorizer.vocabulary_.items()  # les bi-grammes et leurs indices (rangs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1.3 Matrice TFIDF sur cet exemple\n",
    "**On a vu un exemple avec le comptage du nombre d'occurrence (mono & bi-grammes)**  \n",
    "**On continue avec un exemple de TfIdf simple (comme on avait fait avec les occurrences)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>be</th>\n",
       "      <th>document</th>\n",
       "      <th>first</th>\n",
       "      <th>fourth</th>\n",
       "      <th>from</th>\n",
       "      <th>is</th>\n",
       "      <th>one</th>\n",
       "      <th>or</th>\n",
       "      <th>second</th>\n",
       "      <th>seems</th>\n",
       "      <th>the</th>\n",
       "      <th>third</th>\n",
       "      <th>this</th>\n",
       "      <th>to</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.453491</td>\n",
       "      <td>0.560151</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.453491</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.370758</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.370758</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.415375</td>\n",
       "      <td>0.530257</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.327487</td>\n",
       "      <td>0.415375</td>\n",
       "      <td>0.216760</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.216760</td>\n",
       "      <td>0.415375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.559434</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.357079</td>\n",
       "      <td>0.441064</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.291936</td>\n",
       "      <td>0.441064</td>\n",
       "      <td>0.291936</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.193428</td>\n",
       "      <td>0.238922</td>\n",
       "      <td>0.303042</td>\n",
       "      <td>0.303042</td>\n",
       "      <td>0.193428</td>\n",
       "      <td>0.477844</td>\n",
       "      <td>0.303042</td>\n",
       "      <td>0.238922</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.474420</td>\n",
       "      <td>0.238922</td>\n",
       "      <td>0.158140</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        and        be  document     first    fourth      from        is  \\\n",
       "0  0.000000  0.000000  0.453491  0.560151  0.000000  0.000000  0.453491   \n",
       "1  0.000000  0.415375  0.530257  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.559434  0.000000  0.000000  0.000000  0.000000  0.000000  0.357079   \n",
       "3  0.000000  0.000000  0.193428  0.238922  0.303042  0.303042  0.193428   \n",
       "\n",
       "        one        or    second     seems       the     third      this  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.370758  0.000000  0.370758   \n",
       "1  0.000000  0.000000  0.327487  0.415375  0.216760  0.000000  0.216760   \n",
       "2  0.441064  0.000000  0.000000  0.000000  0.291936  0.441064  0.291936   \n",
       "3  0.477844  0.303042  0.238922  0.000000  0.474420  0.238922  0.158140   \n",
       "\n",
       "         to  \n",
       "0  0.000000  \n",
       "1  0.415375  \n",
       "2  0.000000  \n",
       "3  0.000000  "
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "matrice = pd.DataFrame(X.toarray(),columns=vectorizer.get_feature_names_out())#  get_feature_names, get_feature_names_out pour Python 3.9\n",
    "matrice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\"> Fin de l'exmples simple d'illustration et retour au corpus.</font>\n",
    "---\n",
    "* * *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**On reviens à la BD des sentiments (avis sur les achats)**      \n",
    "<font size=\"4\">Traitement des données de notre corpus étape par étape </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 : Stemming et Lemmatization  de notre corpus\n",
    "\n",
    "**Pouruoi fire ? :  on filtre les mots (lemmatization) pour ne garder que les mots significatifs, ensuite on calcule la matrice TfIdf**\n",
    "\n",
    "Ces deux actions peuvent être regroupées dans un **pipeline**    \n",
    "On importe d'abord quelques outils."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importation du nécessaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False : # SI déjà fait, ne pas refaire\n",
    "    !pip install textblob\n",
    "    import nltk\n",
    "    nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to /home/alex/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('reuters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "\n",
    "# Une seule fois :\n",
    "if False : # Si déjà fit\n",
    "    nltk.download('wordnet')\n",
    "    nltk.download('words')\n",
    "    nltk.download('punkt')    \n",
    "    nltk.download('averaged_perceptron_tagger')\n",
    "    nltk.download('brown')\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('omw-1.4')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob, Word\n",
    "\n",
    "import string\n",
    "\n",
    "# Initialisation du \"Wordnet Lemmatizer\"\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "#Pour tester :  print(lemmatizer.lemmatize(\"bats\"))\n",
    "\n",
    "\n",
    "from nltk.corpus import brown  # Il y a davantage de mots ici\n",
    "words = set(brown.words())\n",
    "\n",
    "#words = set(nltk.corpus.words.words()) : pas beaucoup de mots !\n",
    "stop_words=set(stopwords.words('english')); # \";\" pour ne pas avoir les résultats !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.2.1 Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__On définit notre lemmatizer qui pré-traitera  le corpus!__\n",
    "1) vérifier que nos \"mots\" sont parmi les mots acceptables (non \"usuels\") : dans \"words\"\n",
    "2) enlever les mots usuels (stopwords)\n",
    "3) enlever qq mots étranges\n",
    "4) enlever les ponctuations\n",
    "5) etc.\n",
    "6) Lemmatiser !    \n",
    "\n",
    "N.B. 'le' et 'u' sont mis \"à la main\" !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemma(texte) :\n",
    "    #renvoie lemmatizer.lemmatize(texte)\n",
    "    return [lemmatizer.lemmatize(t) for t in word_tokenize(texte) if \\\n",
    "            t.lower() in words and \\\n",
    "            t.lower() not in stop_words \\\n",
    "            # cas des strs spécifiques non filtrés\n",
    "            and t not in [\"''\", '--', '1.2', '1/2', '18th', '2-3', '20th', '4.00', '4.2', '``', 'le', 'u']\\\n",
    "            #and t.lower() not in word_tokenize(stop_words).encode() \\ # génère un pb de 'byte' ?!\n",
    "            and t.lower() not in string.punctuation and not t.isdigit()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<font color=\"red\"> Ici : si warning sur stopwords, lancer cette cellule une 2e fois </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Appliquer puis afficher quelques informations sont le corpus restant__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/venv-conda-3.9/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/alex/venv-conda-3.9/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['le', 'u'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il y a  3201  termes dans le vocabulaire\n",
      "\n",
      "Les 50 premiers termes:\n",
      "['abandoned' 'ability' 'able' 'abound' 'abroad' 'absolute' 'absolutely'\n",
      " 'abysmal' 'academy' 'accent' 'accept' 'acceptable' 'access' 'accessible'\n",
      " 'accessory' 'accident' 'accidentally' 'acclaimed' 'accolade'\n",
      " 'accommodation' 'accompanied' 'according' 'accordingly' 'accountant'\n",
      " 'accurate' 'accurately' 'accused' 'ache' 'achievement' 'acknowledged'\n",
      " 'act' 'acted' 'acting' 'action' 'activate' 'activated' 'actor' 'actress'\n",
      " 'actual' 'actually' 'ad' 'adaptation' 'adapter' 'add' 'added' 'addition'\n",
      " 'additional' 'address' 'adhesive' 'admiration']\n",
      "\n",
      "Tous les 50 termes:\n",
      "['abandoned' 'admitted' 'annoying' 'astonishingly' 'banana' 'bird' 'bread'\n",
      " 'buying' 'cellular' 'church' 'comfortably' 'connected' 'cord' 'critic'\n",
      " 'decor' 'destroying' 'displeased' 'dropping' 'emotion' 'everybody'\n",
      " 'extensive' 'feeling' 'focused' 'frozen' 'glass' 'gyro' 'heroine' 'house'\n",
      " 'impulse' 'instead' 'joke' 'laugh' 'list' 'loyalty' 'masterpiece'\n",
      " 'miserable' 'narration' 'notice' 'order' 'painted' 'performing' 'plenty'\n",
      " 'preferably' 'propaganda' 'r' 'recognizes' 'remorse' 'ridiculous'\n",
      " 'sandwich' 'securely' 'sheer' 'sits' 'son' 'starlet' 'structure'\n",
      " 'support' 'teacher' 'thumb' 'trailer' 'unacceptable' 'unrealistic'\n",
      " 'video' 'washing' 'wish' 'zombie']\n",
      "\n",
      "Le nbr d'occurrence des 50 premiers termes :\n",
      "[('way', 3114), ('plug', 2054), ('unless', 2993), ('good', 1210), ('case', 383), ('excellent', 961), ('value', 3034), ('great', 1226), ('jawbone', 1490), ('tied', 2856), ('conversation', 590), ('lasting', 1543), ('problem', 2130), ('line', 1597), ('right', 2353), ('decent', 696), ('volume', 3075), ('dozen', 825), ('contact', 566), ('imagine', 1384), ('fun', 1160), ('sending', 2463), ('owner', 1935), ('needle', 1818), ('say', 2412), ('wasted', 3105), ('money', 1768), ('waste', 3104), ('time', 2859), ('sound', 2612), ('quality', 2192), ('impressed', 1393), ('going', 1207), ('original', 1908), ('battery', 219), ('extended', 999), ('mere', 1732), ('started', 2655), ('notice', 1852), ('excessive', 967), ('static', 2660), ('garbled', 1177), ('design', 739), ('odd', 1871), ('ear', 860), ('clip', 469), ('comfortable', 499), ('highly', 1310), ('recommend', 2253), ('blue', 269)]\n",
      "\n",
      "Le nbr d'occurrence des 50 premiers termes :\n",
      "[('occasion', 1865), ('bloodiest', 265), ('anymore', 105), ('allergy', 74), ('clue', 476), ('contain', 569), ('unprofessional', 3001), ('loyal', 1650), ('patron', 1979), ('occasional', 1866), ('anticipated', 103), ('concept', 534), ('poisoning', 2064), ('batch', 216), ('eve', 946), ('caring', 377), ('teamwork', 2804), ('degree', 712), ('downright', 823), ('hurry', 1362), ('reservation', 2326), ('stretch', 2694), ('ranch', 2207), ('dipping', 770), ('watered', 3111), ('garden', 1178), ('spotty', 2642), ('ensued', 921), ('apologize', 110), ('binge', 248), ('drinking', 844), ('profound', 2142), ('combo', 495), ('cart', 381), ('blame', 257), ('del', 713), ('hamburger', 1258), ('correction', 607), ('brownish', 321), ('ha', 1252), ('bigger', 246), ('sub', 2718), ('mile', 1743), ('brushfire', 323), ('dried', 840), ('caterpillar', 391), ('appetite', 117), ('instantly', 1450), ('poured', 2089), ('wound', 3182)]\n"
     ]
    }
   ],
   "source": [
    "count_vec_lemmatise = CountVectorizer(tokenizer=lemma, stop_words=\"english\", analyzer='word', \n",
    "                            ngram_range=(1, 1), max_df=1.0, min_df=0, max_features=None)\n",
    "\n",
    "# Transformer les données en  bag of words\n",
    "count_train = count_vec_lemmatise.fit(df_papers[\"Avis\"])\n",
    "bag_of_words_of_corpus = count_vec_lemmatise.transform(df_papers[\"Avis\"])\n",
    "\n",
    "# On enlève qq termes inutiles qui nous ont échappés (qui ont été créés par lemmatize)\n",
    "# Il s'agit d'un Dict de Python.\n",
    "for terme_a_jeter in ['n', 'u', 'ft'] :\n",
    "    count_train.vocabulary_.pop(terme_a_jeter, terme_a_jeter+\" n'y est ps !\")\n",
    "\n",
    "\n",
    "# Quelques prints \n",
    "print(\"Il y a \", len(count_train.vocabulary_), \" termes dans le vocabulaire\\n\")\n",
    "\n",
    "# Print Les 50 premiers termes\n",
    "print(\"Les 50 premiers termes:\\n{}\".format(np.array(count_vec_lemmatise.get_feature_names_out()[:50])))\n",
    "print(\"\\nTous les 50 termes:\\n{}\".format(np.array(count_vec_lemmatise.get_feature_names_out()[::50]))) # Tous les 50 termes\n",
    "#print(type(count_train.vocabulary_))\n",
    "#print(\"Vocabulary content:\\n {}\".format(count_train.vocabulary_))\n",
    "\n",
    "#les 50 premiers mots et leur nbr d'occurrence\n",
    "print(\"\\nLe nbr d'occurrence des 50 premiers termes :\")\n",
    "print([(k,v)  for k,v in count_train.vocabulary_.items()][:50])\n",
    "\n",
    "print(\"\\nLe nbr d'occurrence des 50 premiers termes :\")\n",
    "print([(k,v)  for k,v in count_train.vocabulary_.items()][-50:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\"> On peut faire la même chose avec une classe (Comptage du nbr d'occurrences) avec `lemma` du package  `pattern`</font>\n",
    "\n",
    "_On note que les résultats ne sont pas tout à fait les mêmes (pas les mêmes packages, pas les même dicos)_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Rappel : On a déjà fait le nécessaire plus hatut. </font>\n",
    "\n",
    "De plus, on a besoin du module \"pattern\" : difficile à installer sur certaines machnes (selon les versions de jupyter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False : # Mettre True si vou sn'avez jamais installé ce package\n",
    "    !pip install pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\" size=\"3\"> Si warning sur stopwords, lancer cette cellule une 2e fois </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les 50 premiers termes:\n",
      "['abandon' 'abhor' 'ability' 'able' 'abound' 'abroad' 'absolute'\n",
      " 'absolutely' 'abstruse' 'abysmal' 'academy' 'accept' 'acceptable'\n",
      " 'access' 'accessible' 'accessory' 'accident' 'accidentally' 'accord'\n",
      " 'accordingly' 'accountant' 'accurate' 'accurately' 'accuse' 'ache'\n",
      " 'achievement' 'acknowledge' 'acros' 'act' 'action' 'activate' 'actor'\n",
      " 'actres' 'actual' 'actually' 'ad' 'adaptation' 'adapter' 'add' 'addition'\n",
      " 'additional' 'address' 'adhesive' 'admiration' 'admit' 'adorable'\n",
      " 'adrift' 'adventure' 'advise' 'aerial']\n",
      "\n",
      "Tous les 50 termes:\n",
      "['abandon' 'aesthetically' 'appeal' 'award' 'beauty' 'bold' 'bulky'\n",
      " 'catch' 'chow' 'comfortable' 'contain' 'crack' 'data' 'despite' 'distort'\n",
      " 'duo' 'end' 'exchange' 'far' 'flawlessly' 'frontier' 'gluten' 'halibut'\n",
      " 'hip' 'hut' 'indoor' 'ironside' 'large' 'lino' 'magnetic' 'meet'\n",
      " 'monotonou' 'neighborhood' 'older' 'palm' 'phantasm' 'point' 'president'\n",
      " 'puree' 'receive' 'replacement' 'rotate' 'screamy' 'shame' 'skil' 'sort'\n",
      " 'stay' 'subject' 'syrupy' 'thinly' 'traditional' 'unacceptable'\n",
      " 'unpleasant' 'verbatim' 'waste' 'woo']\n",
      "\n",
      "Le nbr d'occurrence des 50 premiers termes :\n",
      "[('way', 2706), ('plug', 1794), ('u', 2547), ('unles', 2591), ('converter', 515), ('good', 1055), ('case', 343), ('excellent', 844), ('value', 2638), ('great', 1072), ('jawbone', 1305), ('tie', 2465), ('charger', 373), ('jiggle', 1315), ('line', 1398), ('right', 2032), ('decent', 613), ('volume', 2680), ('dozen', 724), ('hundr', 1195), ('imagine', 1216), ('fun', 1007), ('send', 2125), ('owner', 1688), ('needles', 1595), ('say', 2083), ('waste', 2700), ('money', 1549), ('time', 2468), ('sound', 2252), ('quality', 1908), ('original', 1664), ('battery', 193), ('extend', 871), ('mere', 1513), ('notice', 1625), ('excessive', 848), ('static', 2298), ('headset', 1123), ('design', 644), ('odd', 1641), ('ear', 755), ('clip', 419), ('comfortable', 450), ('highly', 1145), ('recommend', 1956), ('blue', 244), ('tooth', 2489), ('phone', 1754), ('advise', 48)]\n"
     ]
    }
   ],
   "source": [
    "# On a déjà fait le nécessaire plus hatut. \n",
    "# On met un \"if False\" pour ne pas exécuter sauf ...\n",
    "# Eventuellement NE pas utiliser : plus lourd à installer\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "if True : # Mettre 3false\" pour ne pas afficher des messages d'erreur \n",
    "    # Il y a des problèmes avec la ligne suivante sur certaine machines\n",
    "    from pattern.en import lemma,lexeme\n",
    "    import nltk\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "    class LemmaTokenizer(object):\n",
    "        def __init__(self):\n",
    "            self.wnl = WordNetLemmatizer()\n",
    "        def __call__(self, articles):\n",
    "            #return [lemma(t) for t in word_tokenize(articles) if t.lower() in words]\n",
    "\n",
    "            return [lemma(t) for t in word_tokenize(articles) if t.lower() in words and \\\n",
    "                    t.lower() not in stop_words \\\n",
    "                    #and t.lower() not in word_tokenize(stop_words).encode() \\ # génère un pb de 'byte' ?!\n",
    "                    # cas des strs spécifiques non filtrés\n",
    "                    and t not in [\"''\", '--', '1.2', '1/2', '18th', '2-3', '20th', '4.00', '4.2', '``']\\\n",
    "                    and t.lower() not in string.punctuation and not t.isdigit()]\n",
    "\n",
    "        \n",
    "        \n",
    "    words = set(nltk.corpus.words.words())\n",
    "\n",
    "    count_vec_lemmatise = CountVectorizer(tokenizer=LemmaTokenizer(), stop_words=\"english\", analyzer='word', \n",
    "                                ngram_range=(1, 1), max_df=1.0, min_df=1, max_features=None)\n",
    "\n",
    "    # Transforms the data into a bag of words\n",
    "    count_train = count_vec_lemmatise.fit(df_papers[\"Avis\"])\n",
    "    bag_of_words = count_vec_lemmatise.transform(df_papers[\"Avis\"])\n",
    "\n",
    "    # Print the first 10 features of the count_vec\n",
    "    print(\"Les 50 premiers termes:\\n{}\".format(count_vec_lemmatise.get_feature_names_out()[:50]))\n",
    "    print(\"\\nTous les 50 termes:\\n{}\".format(count_vec_lemmatise.get_feature_names_out()[::50])) # Tous les 50 termes\n",
    "    #print(type(count_train.vocabulary_))\n",
    "    #print(\"Vocabulary content:\\n {}\".format(count_train.vocabulary_))\n",
    "\n",
    "    #les 50 premiers mots et leur nbr d'occurrence\n",
    "    print(\"\\nLe nbr d'occurrence des 50 premiers termes :\")\n",
    "    print([(k,v)  for k,v in count_train.vocabulary_.items()][:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3- Etape TfIdf\n",
    "Commencons d'abord avec un exemple simple.\n",
    "#### 2.2.3.1 Un exemple simple        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary_ :  {'the': 7, 'quick': 6, 'brown': 0, 'fox': 2, 'jumped': 3, 'over': 5, 'lazy': 4, 'yellow': 8, 'dog': 1}\n",
      "idf_ :  [1.69314718 1.28768207 1.28768207 1.69314718 1.69314718 1.69314718\n",
      " 1.69314718 1.         1.69314718]\n",
      "(1, 9)\n",
      "[[0.34195062 0.26006226 0.26006226 0.34195062 0.34195062 0.34195062\n",
      "  0.34195062 0.40392309 0.34195062]]\n"
     ]
    }
   ],
   "source": [
    "# Pour comprendre\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# liste de  documents\n",
    "text = [\"The quick brown fox jumped over the lazy yellow dog.\", \"The dog.\", \"The fox\"]\n",
    "# Création de transform\n",
    "vectorizer = TfidfVectorizer(max_df=10, min_df=1)\n",
    "# tokenize et construire le vocab\n",
    "vectorizer.fit(text)\n",
    "# summarize\n",
    "print(\"vocabulary_ : \" , vectorizer.vocabulary_)\n",
    "print(\"idf_ : \" ,vectorizer.idf_)\n",
    "# encode document\n",
    "vector = vectorizer.transform([text[0]])\n",
    "# summarize encoded vector\n",
    "print(vector.shape)\n",
    "print(vector.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explications**\n",
    "\n",
    "Ici un vocabulaire de 9 mots a été appris.  Il y a 9 mots (termes) différents.\n",
    "\n",
    "Chaque mot est assigné à un entier (son indice) dans le vecteur de réels en sortie des calculs de TfIdf (ici \"vectorizer\").\n",
    "\n",
    "Le Idf (inverse document frequency minimal) est ici =1 qui a été calculé pour le 7e mot (\"the\", en partant de 0). \"The\" est le mot le plus fréquent et donc son inverse (Idf) est le pus petit !\n",
    "La table est donc normalisée (min Idf =1.0)\n",
    "\n",
    "On a ensuite demandé à encoder le premier \"document\" (la première phrase de la liste avec 9 mots différents) et le résultat est un vecteur de 9 réels de TfIdf (cette fois) pour les différents mots de cette phrase.\n",
    "On constate que les mots “the“, “fox“, et “dog” ont des valeurs différentes par rapport aux autres mots de la phrase (qui sont tous à 0.34195062)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fin Exemple\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3.2 **Calcul TfIdf de notre corpus**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\" size=\"3\"> Si vous avez installé \"Pattern\", vou spouvez changer la 1ere ligne de 1a cellule\n",
    "suivante par :      \n",
    "    \n",
    "TfIdf_lemmatise = TfidfVectorizer(tokenizer=lemma, stop_words=\"english\",\\\n",
    "     smooth_idf=False, sublinear_tf=False, norm=None, analyzer='word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/venv-conda-3.9/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/alex/venv-conda-3.9/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['acros', 'afterward', 'alway', 'anythe', 'everythe', 'hundr', 'indee', 'les', 'make', 'nevertheles', 'nothe', 'perhap', 'seriou', 'somethe', 'thu', 'u', 'wherea'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) Les 10 dernières lignes de la  matrice TfIdf (en ligne : les indices, en colonne les termes/mots) :\n",
      "      abandon  abhor  ability  able  abound  abroad  absolute  absolutely  \\\n",
      "2240      0.0    0.0      0.0   0.0     0.0     0.0       0.0         0.0   \n",
      "2241      0.0    0.0      0.0   0.0     0.0     0.0       0.0         0.0   \n",
      "2242      0.0    0.0      0.0   0.0     0.0     0.0       0.0         0.0   \n",
      "2243      0.0    0.0      0.0   0.0     0.0     0.0       0.0         0.0   \n",
      "2244      0.0    0.0      0.0   0.0     0.0     0.0       0.0         0.0   \n",
      "2245      0.0    0.0      0.0   0.0     0.0     0.0       0.0         0.0   \n",
      "2246      0.0    0.0      0.0   0.0     0.0     0.0       0.0         0.0   \n",
      "2247      0.0    0.0      0.0   0.0     0.0     0.0       0.0         0.0   \n",
      "2248      0.0    0.0      0.0   0.0     0.0     0.0       0.0         0.0   \n",
      "2249      0.0    0.0      0.0   0.0     0.0     0.0       0.0         0.0   \n",
      "\n",
      "      abstruse  abysmal  ...  yellow  yellowtail  young  younger  youthful  \\\n",
      "2240       0.0      0.0  ...     0.0         0.0    0.0      0.0       0.0   \n",
      "2241       0.0      0.0  ...     0.0         0.0    0.0      0.0       0.0   \n",
      "2242       0.0      0.0  ...     0.0         0.0    0.0      0.0       0.0   \n",
      "2243       0.0      0.0  ...     0.0         0.0    0.0      0.0       0.0   \n",
      "2244       0.0      0.0  ...     0.0         0.0    0.0      0.0       0.0   \n",
      "2245       0.0      0.0  ...     0.0         0.0    0.0      0.0       0.0   \n",
      "2246       0.0      0.0  ...     0.0         0.0    0.0      0.0       0.0   \n",
      "2247       0.0      0.0  ...     0.0         0.0    0.0      0.0       0.0   \n",
      "2248       0.0      0.0  ...     0.0         0.0    0.0      0.0       0.0   \n",
      "2249       0.0      0.0  ...     0.0         0.0    0.0      0.0       0.0   \n",
      "\n",
      "      yucky  yummy    z  zero  zombie  \n",
      "2240    0.0    0.0  0.0   0.0     0.0  \n",
      "2241    0.0    0.0  0.0   0.0     0.0  \n",
      "2242    0.0    0.0  0.0   0.0     0.0  \n",
      "2243    0.0    0.0  0.0   0.0     0.0  \n",
      "2244    0.0    0.0  0.0   0.0     0.0  \n",
      "2245    0.0    0.0  0.0   0.0     0.0  \n",
      "2246    0.0    0.0  0.0   0.0     0.0  \n",
      "2247    0.0    0.0  0.0   0.0     0.0  \n",
      "2248    0.0    0.0  0.0   0.0     0.0  \n",
      "2249    0.0    0.0  0.0   0.0     0.0  \n",
      "\n",
      "[10 rows x 2786 columns]\n",
      "--------------------------------------------------\n",
      "2) Quelques indice Idf pour certains mots :\n",
      "         idf_weights\n",
      "abandon     9.006368\n",
      "abhor       9.006368\n",
      "ability     7.396930\n",
      "able        7.620073\n",
      "abound      9.006368\n",
      "...              ...\n",
      "yucky       9.006368\n",
      "yummy       7.620073\n",
      "z           9.006368\n",
      "zero        7.396930\n",
      "zombie      9.006368\n",
      "\n",
      "[2786 rows x 1 columns]\n",
      "--------------------------------------------------\n",
      "2bis) Le 100 premiers termes : colonnes de la matrice TfIdf\n",
      "['abandon' 'abhor' 'ability' 'able' 'abound' 'abroad' 'absolute'\n",
      " 'absolutely' 'abstruse' 'abysmal' 'academy' 'accept' 'acceptable'\n",
      " 'access' 'accessible' 'accessory' 'accident' 'accidentally' 'accord'\n",
      " 'accordingly' 'accountant' 'accurate' 'accurately' 'accuse' 'ache'\n",
      " 'achievement' 'acknowledge' 'acros' 'act' 'action' 'activate' 'actor'\n",
      " 'actres' 'actual' 'actually' 'ad' 'adaptation' 'adapter' 'add' 'addition'\n",
      " 'additional' 'address' 'adhesive' 'admiration' 'admit' 'adorable'\n",
      " 'adrift' 'adventure' 'advise' 'aerial' 'aesthetically' 'affect'\n",
      " 'affordable' 'afraid' 'afternoon' 'age' 'aggravate' 'ago' 'agree' 'ahead'\n",
      " 'aimles' 'air' 'airport' 'akin' 'ala' 'alarm' 'alert' 'alike' 'allergy'\n",
      " 'allot' 'allow' 'alongside' 'aluminum' 'alway' 'amateurish' 'amaze'\n",
      " 'amazingly' 'ambience' 'ample' 'amuse' 'anatomist' 'angel' 'angle'\n",
      " 'angry' 'anguish' 'animate' 'animation' 'ann' 'anniversary' 'annoy'\n",
      " 'answer' 'ant' 'antithesi' 'anythe' 'apart' 'apartment' 'apologize'\n",
      " 'apology' 'appall' 'apparently']\n",
      "--------------------------------------------------\n",
      "3) Le 100 derniers termes de la matrice = colonnes de la matrice\n",
      "['waitres' 'wake' 'walk' 'wall' 'wallet' 'want' 'war' 'warm' 'warmer'\n",
      " 'warmth' 'warn' 'warranty' 'wartime' 'wash' 'waste' 'watch' 'watchable'\n",
      " 'water' 'waterproof' 'wave' 'way' 'waylay' 'weak' 'wear' 'weave' 'web'\n",
      " 'wed' 'week' 'weekend' 'weekly' 'weight' 'weird' 'welcome' 'welsh'\n",
      " 'whatsoever' 'whine' 'whiny' 'white' 'whoa' 'wholesome' 'wide' 'wife'\n",
      " 'wild' 'wildly' 'willie' 'wily' 'win' 'wind' 'window' 'wine' 'winner'\n",
      " 'wire' 'wireless' 'wise' 'wish' 'wit' 'witty' 'wo' 'wobbly' 'wonder'\n",
      " 'wonderful' 'wonderfully' 'wong' 'wont' 'woo' 'wood' 'wooden' 'word'\n",
      " 'work' 'worker' 'world' 'worry' 'worse' 'worst' 'worth' 'worthles'\n",
      " 'worthy' 'wouldnt' 'wow' 'wrap' 'wrapped' 'write' 'writer' 'wrong'\n",
      " 'wrongly' 'yawn' 'ye' 'yeah' 'year' 'yell' 'yellow' 'yellowtail' 'young'\n",
      " 'younger' 'youthful' 'yucky' 'yummy' 'z' 'zero' 'zombie']\n",
      "--------------------------------------------------\n",
      "Il y a 2400 documents, 3204 termes \n",
      "4) La matrice sur X_train : \n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "--------------------------------------------------\n",
      "5) Les 50 premiers termes du vocabulaire  et leur indices :\n",
      "[('way', 2706), ('plug', 1794), ('u', 2547), ('unles', 2591), ('converter', 515), ('good', 1055), ('case', 343), ('excellent', 844), ('value', 2638), ('great', 1072), ('jawbone', 1305), ('tie', 2465), ('charger', 373), ('jiggle', 1315), ('line', 1398), ('right', 2032), ('decent', 613), ('volume', 2680), ('dozen', 724), ('hundr', 1195), ('imagine', 1216), ('fun', 1007), ('send', 2125), ('owner', 1688), ('needles', 1595), ('say', 2083), ('waste', 2700), ('money', 1549), ('time', 2468), ('sound', 2252), ('quality', 1908), ('original', 1664), ('battery', 193), ('extend', 871), ('mere', 1513), ('notice', 1625), ('excessive', 848), ('static', 2298), ('headset', 1123), ('design', 644), ('odd', 1641), ('ear', 755), ('clip', 419), ('comfortable', 450), ('highly', 1145), ('recommend', 1956), ('blue', 244), ('tooth', 2489), ('phone', 1754), ('advise', 48)]\n",
      "Voici les vecteurs de quelques termes :\n",
      "\n",
      "valeur de Zero 2784\n",
      "      yucky\n",
      "0       0.0\n",
      "1       0.0\n",
      "2       0.0\n",
      "3       0.0\n",
      "4       0.0\n",
      "...     ...\n",
      "2245    0.0\n",
      "2246    0.0\n",
      "2247    0.0\n",
      "2248    0.0\n",
      "2249    0.0\n",
      "\n",
      "[2250 rows x 1 columns]\n",
      "       young\n",
      "0        0.0\n",
      "1        0.0\n",
      "2        0.0\n",
      "3        0.0\n",
      "4        0.0\n",
      "...      ...\n",
      "2245     0.0\n",
      "2246     0.0\n",
      "2247     0.0\n",
      "2248     0.0\n",
      "2249     0.0\n",
      "\n",
      "[2250 rows x 1 columns]\n",
      "      yellow\n",
      "0        0.0\n",
      "1        0.0\n",
      "2        0.0\n",
      "3        0.0\n",
      "4        0.0\n",
      "...      ...\n",
      "2245     0.0\n",
      "2246     0.0\n",
      "2247     0.0\n",
      "2248     0.0\n",
      "2249     0.0\n",
      "\n",
      "[2250 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "TfIdf_lemmatise = TfidfVectorizer(tokenizer=LemmaTokenizer(), stop_words=\"english\", \n",
    "                    smooth_idf=False, sublinear_tf=False, norm=None, analyzer='word')\n",
    "# max_df = min_df : par default=1.0\n",
    "\n",
    "corpus_fitted = TfIdf_lemmatise.fit(df_papers[\"Avis\"])\n",
    "train_lemmatise_transformed = corpus_fitted.transform(X_train[\"Avis\"])\n",
    "test_lemmatise_transformed = corpus_fitted.transform(X_test[\"Avis\"])\n",
    "\n",
    "# Une partie de la matrice TDIDF\n",
    "temp_df=pd.DataFrame(train_lemmatise_transformed.toarray(), columns=TfIdf_lemmatise.get_feature_names_out())\n",
    "\n",
    "#print(temp_df.columns.values)\n",
    "print(\"1) Les 10 dernières lignes de la  matrice TfIdf (en ligne : les indices, en colonne les termes/mots) :\")\n",
    "print(temp_df.tail(10))\n",
    "print( \"-\"*50)\n",
    "\n",
    "df_idf = pd.DataFrame(TfIdf_lemmatise.idf_, index=TfIdf_lemmatise.get_feature_names_out(),columns=[\"idf_weights\"])\n",
    "print(\"2) Quelques indice Idf pour certains mots :\")\n",
    "print(df_idf)\n",
    "print(\"-\"*50)\n",
    "\n",
    "print(\"2bis) Le 100 premiers termes : colonnes de la matrice TfIdf\")\n",
    "liste_termes=TfIdf_lemmatise.get_feature_names_out()\n",
    "print(liste_termes[:100])\n",
    "print(\"-\"*50)\n",
    "\n",
    "print(\"3) Le 100 derniers termes de la matrice = colonnes de la matrice\")\n",
    "print(liste_termes[-100:])\n",
    "print(\"-\"*50)\n",
    "\n",
    "print(\"Il y a 2400 documents, 3204 termes \")\n",
    "print (\"4) La matrice sur X_train : \\n\", train_lemmatise_transformed.toarray())\n",
    "\n",
    "print(\"-\"*50)\n",
    "print(\"5) Les 50 premiers termes du vocabulaire  et leur indices :\")\n",
    "print([(k,v)  for k,v in TfIdf_lemmatise.vocabulary_.items()][:50])\n",
    "\n",
    "print(\"Voici les vecteurs de quelques termes :\")\n",
    "try : # Suivant le mot donné, il y a erreur si le mot n'existe pas\n",
    "    print(\"\\nvaleur de Zero\", TfIdf_lemmatise.vocabulary_[\"zero\"])\n",
    "    TfIdf_lemmatise.vocabulary_['zero']\n",
    "except : \n",
    "     print(\"Certains mots / temes n'existent pas\")\n",
    "        \n",
    "try : # Suivant le mot donné, il y a erreur si le mot n'existe pas\n",
    "    print(pd.DataFrame([temp_df[\"yucky\"]], index=[\"yucky\"]).T)\n",
    "except : \n",
    "     print(\"Certains mots / temes n'existent pas\")\n",
    "\n",
    "try : # Suivant le mot donné, il y a erreur si le mot n'existe pas\n",
    "    print(pd.DataFrame([temp_df[\"young\"]], index=[\" young\"]).T)\n",
    "except : \n",
    "     print(\"Certains mots / temes n'existent pas\")\n",
    "\n",
    "try : # Suivant le mot donné, il y a erreur si le mot n'existe pas\n",
    "    print(pd.DataFrame([temp_df[\"yellow\"]],index=[\"yellow\"]).T)\n",
    "except : \n",
    "     print(\"Certains mots / temes n'existent pas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\"> On a maintenant la matrice TdIdf.</font>\n",
    "On peut déjà faire des calculs de similarité avec.  \n",
    "**Commençons d'abord un exemple avec une BD de news**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.4- Un exemple de calcul de similarité par cosinus\n",
    "\n",
    "__Les données viennent de newsgroupe__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La taille de la matrice :  (11314, 130107)\n",
      "Exemples de lignes de la matrice TfIdf (voir les valeurs non nulles): \n",
      "   (0, 86580)\t0.13157118714240987\n",
      "  (0, 128420)\t0.04278499079283093\n",
      "  (0, 35983)\t0.03770448563619875\n",
      "  (0, 35187)\t0.09353930598317124\n",
      "  (0, 66098)\t0.09785515708314481\n",
      "  (0, 114428)\t0.05511105154696676\n",
      "  (0, 78955)\t0.05989856888061599\n",
      "  (0, 94362)\t0.055457031390147224\n",
      "  (0, 76722)\t0.06908779999621749\n",
      "  (0, 57308)\t0.1558717009157704\n",
      "  (0, 62221)\t0.02921527992427867\n",
      "  (0, 128402)\t0.05922294083277842\n",
      "  (0, 67156)\t0.07313443922740179\n",
      "  (0, 123989)\t0.08207027465330353\n",
      "  (0, 90252)\t0.031889368795417566\n",
      "  (0, 63363)\t0.08342748387969037\n",
      "  (0, 78784)\t0.0633940918806495\n",
      "  (0, 96144)\t0.10826904490745741\n",
      "  (0, 128026)\t0.060622095889758885\n",
      "  (0, 109271)\t0.10844724822064673\n",
      "  (0, 51730)\t0.09714744057976722\n",
      "  (0, 86001)\t0.07000411445838192\n",
      "  (0, 83256)\t0.08844382496462173\n",
      "  (0, 113986)\t0.17691750674853082\n",
      "  (0, 37565)\t0.03431760442478462\n",
      "  :\t:\n",
      "  (0, 4605)\t0.06332603952480323\n",
      "  (0, 76032)\t0.019219463052223086\n",
      "  (0, 92081)\t0.09913274493911223\n",
      "  (0, 40998)\t0.0780136819691811\n",
      "  (0, 79666)\t0.10936401252414274\n",
      "  (0, 89362)\t0.06521174306303763\n",
      "  (0, 118983)\t0.037085978050619146\n",
      "  (0, 90379)\t0.019928859956645867\n",
      "  (0, 98949)\t0.16068606055394932\n",
      "  (0, 64095)\t0.03542092427131355\n",
      "  (0, 95162)\t0.03447138409326312\n",
      "  (0, 87620)\t0.035671863140815795\n",
      "  (0, 114731)\t0.14447275512784058\n",
      "  (0, 68532)\t0.07325812342131596\n",
      "  (0, 37780)\t0.38133891259493113\n",
      "  (0, 123984)\t0.036854292634593756\n",
      "  (0, 111322)\t0.01915671802495043\n",
      "  (0, 114688)\t0.06214070986309586\n",
      "  (0, 85354)\t0.03696978508816316\n",
      "  (0, 124031)\t0.10798795154169122\n",
      "  (0, 50527)\t0.054614286588587246\n",
      "  (0, 118280)\t0.2118680720828169\n",
      "  (0, 123162)\t0.2597090245735688\n",
      "  (0, 75358)\t0.35383501349706165\n",
      "  (0, 56979)\t0.057470154074851294\n",
      "\n",
      " TfIdf du 1er Document (tfidf[0:1]) : \n",
      "   (0, 86580)\t0.13157118714240987\n",
      "  (0, 128420)\t0.04278499079283093\n",
      "  (0, 35983)\t0.03770448563619875\n",
      "  (0, 35187)\t0.09353930598317124\n",
      "  (0, 66098)\t0.09785515708314481\n",
      "  (0, 114428)\t0.05511105154696676\n",
      "  (0, 78955)\t0.05989856888061599\n",
      "  (0, 94362)\t0.055457031390147224\n",
      "  (0, 76722)\t0.06908779999621749\n",
      "  (0, 57308)\t0.1558717009157704\n",
      "  (0, 62221)\t0.02921527992427867\n",
      "  (0, 128402)\t0.05922294083277842\n",
      "  (0, 67156)\t0.07313443922740179\n",
      "  (0, 123989)\t0.08207027465330353\n",
      "  (0, 90252)\t0.031889368795417566\n",
      "  (0, 63363)\t0.08342748387969037\n",
      "  (0, 78784)\t0.0633940918806495\n",
      "  (0, 96144)\t0.10826904490745741\n",
      "  (0, 128026)\t0.060622095889758885\n",
      "  (0, 109271)\t0.10844724822064673\n",
      "  (0, 51730)\t0.09714744057976722\n",
      "  (0, 86001)\t0.07000411445838192\n",
      "  (0, 83256)\t0.08844382496462173\n",
      "  (0, 113986)\t0.17691750674853082\n",
      "  (0, 37565)\t0.03431760442478462\n",
      "  :\t:\n",
      "  (0, 4605)\t0.06332603952480323\n",
      "  (0, 76032)\t0.019219463052223086\n",
      "  (0, 92081)\t0.09913274493911223\n",
      "  (0, 40998)\t0.0780136819691811\n",
      "  (0, 79666)\t0.10936401252414274\n",
      "  (0, 89362)\t0.06521174306303763\n",
      "  (0, 118983)\t0.037085978050619146\n",
      "  (0, 90379)\t0.019928859956645867\n",
      "  (0, 98949)\t0.16068606055394932\n",
      "  (0, 64095)\t0.03542092427131355\n",
      "  (0, 95162)\t0.03447138409326312\n",
      "  (0, 87620)\t0.035671863140815795\n",
      "  (0, 114731)\t0.14447275512784058\n",
      "  (0, 68532)\t0.07325812342131596\n",
      "  (0, 37780)\t0.38133891259493113\n",
      "  (0, 123984)\t0.036854292634593756\n",
      "  (0, 111322)\t0.01915671802495043\n",
      "  (0, 114688)\t0.06214070986309586\n",
      "  (0, 85354)\t0.03696978508816316\n",
      "  (0, 124031)\t0.10798795154169122\n",
      "  (0, 50527)\t0.054614286588587246\n",
      "  (0, 118280)\t0.2118680720828169\n",
      "  (0, 123162)\t0.2597090245735688\n",
      "  (0, 75358)\t0.35383501349706165\n",
      "  (0, 56979)\t0.057470154074851294\n",
      "\n",
      " Et son texte : \n",
      " [\"From: lerxst@wam.umd.edu (where's my thing)\\nSubject: WHAT car is this!?\\nNntp-Posting-Host: rac3.wam.umd.edu\\nOrganization: University of Maryland, College Park\\nLines: 15\\n\\n I was wondering if anyone out there could enlighten me on this car I saw\\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\\nthe front bumper was separate from the rest of the body. This is \\nall I know. If anyone can tellme a model name, engine specs, years\\nof production, where this car is made, history, or whatever info you\\nhave on this funky looking car, please e-mail.\\n\\nThanks,\\n- IL\\n   ---- brought to you by your neighborhood Lerxst ----\\n\\n\\n\\n\\n\"]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "twenty = fetch_20newsgroups()\n",
    "tfidf = TfidfVectorizer().fit_transform(twenty.data)\n",
    "print(\"La taille de la matrice : \", tfidf.shape)\n",
    "print(\"Exemples de lignes de la matrice TfIdf (voir les valeurs non nulles): \\n\", tfidf[0:1][:3])\n",
    "\n",
    "print(\"\\n TfIdf du 1er Document (tfidf[0:1]) : \\n\", tfidf[0:1])\n",
    "print(\"\\n Et son texte : \\n\", twenty.data[0:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Similarité entre le 1er doc et les autres**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "cosine_similarities :  [1.         0.04405974 0.11017033 ... 0.04433678 0.04457107 0.0329325 ]\n",
      "--------------------------------------------------\n",
      "\n",
      "La valeur du vecteur related_docs_indices :  [    0   958 10576  3277]\n",
      "\n",
      "Et la valeur du premier document proche  (lui même ?): \n",
      " From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "Thanks,\n",
      "- IL\n",
      "   ---- brought to you by your neighborhood Lerxst ----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Et la valeur du 2e document proche : \n",
      " From: rseymour@reed.edu (Robert Seymour)\n",
      "Subject: Re: WHAT car is this!?\n",
      "Article-I.D.: reed.1993Apr21.032905.29286\n",
      "Reply-To: rseymour@reed.edu\n",
      "Organization: Reed College, Portland, OR\n",
      "Lines: 26\n",
      "\n",
      "In article <1993Apr20.174246.14375@wam.umd.edu> lerxst@wam.umd.edu (where's my  \n",
      "thing) writes:\n",
      "> \n",
      ">  I was wondering if anyone out there could enlighten me on this car I saw\n",
      "> the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "> early 70s. It was called a Bricklin. The doors were really small. In  \n",
      "addition,\n",
      "> the front bumper was separate from the rest of the body. This is \n",
      "> all I know. If anyone can tellme a model name, engine specs, years\n",
      "> of production, where this car is made, history, or whatever info you\n",
      "> have on this funky looking car, please e-mail.\n",
      "\n",
      "Bricklins were manufactured in the 70s with engines from Ford. They are rather  \n",
      "odd looking with the encased front bumper. There aren't a lot of them around,  \n",
      "but Hemmings (Motor News) ususally has ten or so listed. Basically, they are a  \n",
      "performance Ford with new styling slapped on top.\n",
      "\n",
      ">    ---- brought to you by your neighborhood Lerxst ----\n",
      "\n",
      "Rush fan?\n",
      "\n",
      "--\n",
      "Robert Seymour\t\t\t\trseymour@reed.edu\n",
      "Physics and Philosophy, Reed College\t(NeXTmail accepted)\n",
      "Artificial Life Project\t\t\tReed College\n",
      "Reed Solar Energy Project (SolTrain)\tPortland, OR\n",
      "\n",
      "\n",
      "Et la valeur du 3e document proche : \n",
      " From: anuster@wam.umd.edu (Anu Tuli)\n",
      "Subject: Car for Sale\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Distribution: usa\n",
      "Lines: 35\n",
      "\n",
      "From mikefran Wed Apr 21 10:55:39 EDT 1993\n",
      "Article: 56 of csc.general\n",
      "Newsgroups: dc.forsale,dc.general,um.general,csc.general\n",
      "Path: wam.umd.edu!mikefran\n",
      "From: mikefran@wam.umd.edu (Michael Francis)\n",
      "Subject: Car for Sale\n",
      "Message-ID: <1993Apr21.142729.7039@wam.umd.edu>\n",
      "Keywords: 1981 Volkswagon Scirocco\n",
      "Sender: usenet@wam.umd.edu (USENET News system)\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: Workstations at Maryland, University of Maryland, College Park\n",
      "Distribution: csc,um,dc\n",
      "Date: Wed, 21 Apr 1993 14:27:29 GMT\n",
      "\n",
      "\n",
      "1981 Volkswagon Scirocco \n",
      "\n",
      "   Gold exterior and interior\n",
      "   5 speed transmission\n",
      "   AM/FM Stereo with cassette\n",
      "   Sunroof\n",
      "   Engine in good condition\n",
      "   New Tires\n",
      "   Needs $300 work on front left control arm because of damage caused by\n",
      "     pothole.\n",
      "   Runs well \n",
      "   Asking $800.00  AS IS / OBO.\n",
      "\n",
      "   email: mikefran@wam.umd.edu\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calcul de la similarité entre le premier doc et les autres\n",
    "#print(\"Le 1er doc : \", twenty.data[0]) #  le 1er doc le plus similaire sera lui mm (le 1er) !\n",
    "cosine_similarities = linear_kernel(tfidf[0:1], tfidf).flatten()\n",
    "\n",
    "# ou calculer cosin similarities comme ceci :\n",
    "cosine_similarities = cosine_similarity(tfidf[0:1], tfidf).flatten()\n",
    "\n",
    "print(\"\\ncosine_similarities : \", cosine_similarities)\n",
    "related_docs_indices = cosine_similarities.argsort()[:-5:-1]\n",
    "print('-'*50)\n",
    "print(\"\\nLa valeur du vecteur related_docs_indices : \", related_docs_indices)\n",
    "print(\"\\nEt la valeur du premier document proche  (lui même ?): \\n\", twenty.data[related_docs_indices[0]])\n",
    "print(\"\\nEt la valeur du 2e document proche : \\n\", twenty.data[related_docs_indices[1]])\n",
    "print(\"\\nEt la valeur du 3e document proche : \\n\", twenty.data[related_docs_indices[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">   Fin de l'exemple </font>\n",
    "---\n",
    "***\n",
    "<font size=\"4\">  Revenons à nos données :</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Un exemple d'avis :  Nice blanket of moz over top but i feel like this was done to cover up the subpar food.\n",
      "---------------\n",
      "\n",
      "TFIDF du query :   (0, 2501)\t9.006367567650246\n",
      "  (0, 1190)\t6.926926025970411\n",
      "  (0, 1084)\t9.006367567650246\n",
      "  (0, 1055)\t3.635729539522584\n",
      "  (0, 149)\t6.367310238034988\n",
      "  (0, 92)\t9.006367567650246\n",
      "\n",
      "cosine_similarities entre les 2 phrases : [0. 0. 0. ... 0. 0. 0.]\n",
      "\n",
      "elements non. nuls  du consine :  [0.06041407 0.17111881 0.08772926 0.09704747 0.0396816  0.04744736\n",
      " 0.04249041 0.06853495 0.05358078 0.02835221 0.0863332  0.06569773\n",
      " 0.06116887 0.06917064 0.21392788 0.1957163  0.0863332  0.05921539\n",
      " 0.10779273 0.03662838 0.1957163  0.10635041 0.0878677  0.07998539\n",
      " 0.03035502 0.04367694 0.03903531 0.34276102 0.03258619 0.10024324\n",
      " 0.11743218 0.34276102 0.0548005  0.12983349 0.07312283 0.06553252\n",
      " 0.10737983 0.1957163  0.06181918 0.05242053 0.04402211 0.08182257\n",
      " 0.07120751 0.09704747 0.04206988 0.06619942 0.05681388 0.04593757\n",
      " 0.05863438 0.05222576 0.05006667 0.05193926 0.08497611 0.34276102\n",
      " 0.05565903 0.03623678 0.04015475 0.1353016  0.0754091  0.18437295\n",
      " 0.078378   0.07731782 0.03751731 0.09331867 0.34276102 0.03520043\n",
      " 0.06293687 0.09530243 0.03139892 0.08580917 0.05462342 0.12642029\n",
      " 0.12776608 0.04265386 0.04364233 0.05032977 0.06293047 0.09607586\n",
      " 0.1957163  0.04372555 0.15040228 0.05302821 0.10075064 0.10358666\n",
      " 0.34276102 0.06958034 0.29060089 0.09218442 0.0397694  0.09421409\n",
      " 0.07840922 0.1051114  0.06835422 0.0383566  0.07701883 0.05520475\n",
      " 0.04214924 0.06276218 0.06041561 0.12943587 0.11085673 0.07455927\n",
      " 0.03038436 0.08175683 0.07842316 0.05473774 0.11627739 0.09549795\n",
      " 0.11081543 0.05964753 0.04893251 0.10358666 0.18547048 0.05425142\n",
      " 0.12214576 0.06587496 0.05583417 0.06460851 0.14768059 0.05839513\n",
      " 0.07082941 0.02973703 0.11081543 0.05828487 0.03731973 0.04405804\n",
      " 0.04810701 0.04017409 0.05764052 0.04150638 0.0587611  0.05337837\n",
      " 0.07726359 0.06368169 0.07326327 0.08968687 0.06057187 0.04414564\n",
      " 0.06848876 0.04236712 0.0489917  0.07983841 0.06352073 0.05115204\n",
      " 0.11081543 0.06755393 0.03994102 0.0691311  0.02908101 0.05365782\n",
      " 0.10779273 0.04876139 0.05135754 0.05029154 0.08570095 0.07814543\n",
      " 0.1957163  0.03197758 0.03579251 0.02486515 0.04679665 0.06951543\n",
      " 0.04772204 0.12776608 0.05611834 0.07594556 0.04959976 0.025451\n",
      " 0.03327948 0.1957163 ]\n",
      "0.0\n",
      "\n",
      "Les indices des documents proches du query avec similarité par cosinus p/r à train set:\n",
      "[ 427  706  338 1085  824 1149  155  249  997 2219]\n",
      "\n",
      "Le query =  Just whatever you do, avoid \"Groove\" as its the antithesis of all that is good about Human Traffic.  \n",
      "\n",
      "Les documents proches du Query : Just whatever you do, avoid \"Groove\" as its the antithesis of all that is good about Human Traffic.  \n",
      "un docmument proche : Avoid at ALL costs!  \n",
      "un docmument proche : Avoid this one if you can.\n",
      "un docmument proche : Avoid at any and all costs.  \n",
      "un docmument proche : Avoid at all costs.  \n",
      "un docmument proche : Avoid, avoid, avoid!  \n",
      "un docmument proche : Avoid this film at all costs.  \n",
      "un docmument proche : If you hate earbugs, avoid this phone by all means.\n",
      "un docmument proche : There still are good actors around!  \n",
      "un docmument proche : The transfers are very good.  \n",
      "un docmument proche : Good Service-check!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Un exemple de vecteur\n",
    "print(\"Un exemple d'avis : \", np.array(X_test[\"Avis\"])[0])\n",
    "\n",
    "indice_query=1\n",
    "\n",
    "# QQ affichages : on le refait ensuite (voir query_TFIDF)\n",
    "#vecteur = TfIdf_lemmatise.transform([np.array(X_test[\"Avis\"])[indice_query]]) # ZZ : [] important\n",
    "#print(\"\\nLe vecteur TfIdf : \" , vecteur.toarray())\n",
    "#print(\"\\nLe TfIdf du vecteur : \", vecteur)\n",
    "# ZZ : ci-dessous, les affichages donnent les indices aussi. Seul les réels = TfIdf\n",
    "#print(\"\\nET les indices non nuls de ce vecteur : \", end='')\n",
    "#print(np.nonzero(vecteur.toarray())[1])# Les éléments non nul\n",
    "#print(\"**\", np.nonzero(vecteur)[1])# Les éléments non nul\n",
    "print('---------------')\n",
    "\n",
    "# Pour le cas de l'avis  d'indice indice_query\n",
    "query_TFIDF = TfIdf_lemmatise.transform([np.array(X_test[\"Avis\"])[indice_query]])\n",
    "print(\"\\nTFIDF du query :\", query_TFIDF)\n",
    "\n",
    "cosine_similarities = cosine_similarity(query_TFIDF, train_lemmatise_transformed).flatten()\n",
    "related_product_indices = cosine_similarities.argsort()[:-11:-1]\n",
    "print(\"\\ncosine_similarities entre les 2 phrases :\", cosine_similarities)\n",
    "\n",
    "print(\"\\nelements non. nuls  du consine : \", cosine_similarities[np.nonzero(cosine_similarities)])\n",
    "print(cosine_similarities[np.count_nonzero(cosine_similarities, axis=0)])\n",
    "print(\"\\nLes indices des documents proches du query avec similarité par cosinus p/r à train set:\")\n",
    "print(related_product_indices)\n",
    "\n",
    "words= np.array(X_test[\"Avis\"])[indice_query]\n",
    "print(\"\\nLe query = \", words)\n",
    "\n",
    "# Quelques un des documents proches :\n",
    "print(\"\\nLes documents proches du Query :\", np.array(X_test[\"Avis\"])[indice_query])\n",
    "for ind in related_product_indices :\n",
    "    print(\"un docmument proche :\", X_train.iloc[ind][0])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rappel des dimensions de notre corpus (train & test)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2250, 2786), (750, 2786))"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_train=train_lemmatise_transformed\n",
    "fit_test = test_lemmatise_transformed\n",
    "fit_train.shape, fit_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 : Supplément : calcul TfIdF (vactor space) sur le corpus par un Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pour calculer l'indice TfIdf dans le corpus, il faut d'abord compter le nombre d'occurrence du mot puis calculer son Tf puis son Idf et enfin son TfIdf.**\n",
    "\n",
    "Les deux étapes (comptage et TfIdf) peuvent être mises en séquence à l'aide d'un *pipeline*.\n",
    "\n",
    "Pour __\"pipeline\"__, voir https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html\n",
    "\n",
    "__CountVectorizer__ produit un sac de mots (bag of words) : un vecteur avec le nbr d'occurrenecs des mots\n",
    "\n",
    "__TfidfTransformer__ prend ce \"sac\" et le transforme en une matrice avec la fréquence tf-idf (term frequency * inverted document frequency).\n",
    "\n",
    "*make_pipeline* regroupe les deux actions (qui peuvent également être faites séparément)\n",
    "\n",
    "On peut créer des pipelines à multiples étages qui enchainent les actions d'un étage au suivant :\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', LogisticRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">**Remarque**</font>\n",
    "\n",
    "<font color=\"red\">* On a fait le même travail plus haut en Lemmatisant et on a déjà créer fit_train et fit_test</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "if True :\n",
    "    # On a fait le travaille plus haut et déjà créer fit_train et fit_test\n",
    "    pipe = make_pipeline(CountVectorizer(), TfidfTransformer()) # Construire un Construct a Pipeline from the given estimators.\n",
    "    pipe.fit(X_train['Avis'])\n",
    "\n",
    "    fit_train = pipe.transform(X_train['Avis'])\n",
    "    fit_train.shape\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False :\n",
    "    print(fit_train.min(), fit_train.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(750, 4376)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "fit_test = pipe.transform(X_test['Avis'])\n",
    "print(fit_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Partie Analyse \n",
    "### 2.4- Modèle arbre de décision\n",
    "**Un seul arbre de décision produira des résultats assez médiocres.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7133333333333334"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(fit_train, y_train)\n",
    "dt.score(fit_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Le score est assez __médicore__; ce sera une base de comparaison.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5- Modèle Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(n_estimators=50)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(n_estimators=50)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(n_estimators=50)"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=50)\n",
    "clf.fit(fit_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8066666666666666"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(fit_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\"> **Le score s'est un peu amélioré.**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5.1- D'autres métriques d'évaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "score_t = clf.predict_proba(fit_test)\n",
    "score_a = clf.predict_proba(fit_train)\n",
    "fpr_t, tpr_t, seuil_t = roc_curve(y_test, score_t[:, 1])\n",
    "fpr_a, tpr_a, seuil_a = roc_curve(y_train, score_a[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5.2 Courbe ROC (tpr vs. fpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remarquer l'AUC\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "if True :\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(4,4))\n",
    "    ax.plot([0, 1], [0, 1], 'k--')\n",
    "    aucf_t = auc(fpr_t, tpr_t)\n",
    "    ax.plot(fpr_t, tpr_t, label='auc=%1.5f' % aucf_t)\n",
    "    ax.set_title('Courbe ROC - classifieur de Polarités du test')\n",
    "    ax.legend();\n",
    "    \n",
    "print(\"Remarquer l'AUC\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5.3 Comparaison des ROC du train et du test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Les AUCs train/test:', 1.0, 0.8760062293776311)"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "aucf_t = auc(fpr_t, tpr_t)\n",
    "aucf_a = auc(fpr_a, tpr_a)\n",
    "fig, axs = plt.subplots(1, 2, figsize=(8,4))\n",
    "\n",
    "axs[0].plot(fpr_a, tpr_a, label='auc=%1.5f' % aucf_t)\n",
    "axs[0].set_title('Courbe ROC - Apprentissage')\n",
    "axs[1].plot(fpr_t, tpr_t, label='auc=%1.5f' % aucf_a)\n",
    "axs[1].set_title('Courbe ROC - TEST')\n",
    "\n",
    "fig.suptitle('RendomForest :  Courbe ROC du classifieur de Polarités des données')\n",
    "(\"Les AUCs train/test:\", aucf_a, aucf_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. A propose des classifieurs en Text Mining (espace vectoriel) appliqués sur cette DB.\n",
    "-  La transformation du texte en feature génère beaucoup de variables;\n",
    "- __RF__ : Random Forest est une méta méthode aggrégative. Il peut y avoir des problèmes avec RF.\n",
    "    - Intuitivement, il y a un grand risque d'overfitting dans une espace à plusieurs centaines de dimensions, et donc quasiment vide d'échantillons.\n",
    "    - Un des aspects à prendre en compte dans RF est la profondeur de l'arbre. Par défaut, elle est de 10, soit  210=1024  décisions de seuils, soit au mieux  210  variables ce qui est loin du nombre de variables total.\n",
    "\n",
    "- __AD__ :  en présence de beaucoup de variables et un arbre de décision n'exploite quasiment que le fait qu'elles soient non nulles. \n",
    "    - Un arbre de décision consiste à prendre des décisions sur des seuils puis retourne une constante tirée d'une feuille de l'arbre. \n",
    "- __Modèles Linéaires__ : Un modèle linéaire ferait tout aussi bien l'affaire avec en plus la possibilité de tenir compte de la valeur de la variable.\n",
    "    - __MNB__ : le Multinomial Naive Bayes (utilisé pour la détection de spam), qui marche  bien sur BOW (les sacs de mots)\n",
    "    - __Logit__ :  La régression logistique fonctionne assez bien. \n",
    "    - __SVM Linéaire__ : LinearSVM est assez efficace aussi sur ces modèles.\n",
    "\n",
    "- Ces classificateurs améliorent sensiblement les performance des modèles \n",
    "\n",
    "- Les __n-grammes__ n'améliorent pas significativement les performances, \n",
    "- __SVD__ détériore parfois les performances\n",
    "- __word2vect__ améliore légèrement les performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(750, 4376)"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1-__modèle linéaire__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essayons d'abord avec un __modèle linéaire__ (__logit__)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(solver='lbfgs')\n",
    "lr.fit(fit_train, y_train)\n",
    "lr.score(fit_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Une amélioration du score**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2- Multinomial Bayesian (MNB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.856"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(fit_train, y_train)\n",
    "mnb.score(fit_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Encore un bon score**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3- Retour au RandomForest\n",
    "* Si on augmente la profondeur de l'arbre, la forêt aléatoire peut __parfois__ être plus performante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf20 = RandomForestClassifier(n_estimators=120, max_depth=20)\n",
    "clf20.fit(fit_train, y_train)\n",
    "clf20.score(fit_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pas tellement mieux.**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 Itération su Random Forest   \n",
    "Essayons d'autre combinaisons des paramètres dans une itération"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf1,10  : score =  0.475\n",
      "clf1,30  : score =  0.6116666666666667\n",
      "clf1,50  : score =  0.6466666666666666\n",
      "clf1,70  : score =  0.6683333333333333\n",
      "clf1,90  : score =  0.62\n",
      "clf51,10  : score =  0.73\n",
      "clf51,30  : score =  0.775\n",
      "clf51,50  : score =  0.7633333333333333\n",
      "clf51,70  : score =  0.78\n",
      "clf51,90  : score =  0.7766666666666666\n",
      "clf101,10  : score =  0.7533333333333333\n",
      "clf101,30  : score =  0.7833333333333333\n",
      "clf101,50  : score =  0.7566666666666667\n",
      "clf101,70  : score =  0.7983333333333333\n",
      "clf101,90  : score =  0.7766666666666666\n",
      "clf151,10  : score =  0.7666666666666667\n",
      "clf151,30  : score =  0.7916666666666666\n",
      "clf151,50  : score =  0.78\n",
      "clf151,70  : score =  0.7883333333333333\n",
      "clf151,90  : score =  0.7866666666666666\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clfs=[(\"clf\"+str(N)+','+str(depth), RandomForestClassifier(n_estimators=N, max_depth=depth))  for N in range(1,200,50) for depth in range(10,100,20)]\n",
    "for nom, _clf_ in clfs :\n",
    "    _clf_.fit(fit_train, y_train)\n",
    "    print(nom, ' : score = ', _clf_.score(fit_test, y_test))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**La dernière combinaison semble meilleure**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 Une des meilleures combinaisons (?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7866666666666666"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf50_ = RandomForestClassifier(n_estimators=150, max_depth=90)\n",
    "clf50_.fit(fit_train, y_train)\n",
    "clf50_.score(fit_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7766666666666666"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf50_ = RandomForestClassifier(n_estimators=100, max_depth=70)\n",
    "clf50_.fit(fit_train, y_train)\n",
    "clf50_.score(fit_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####3.3.3.  Et une autre (200 arbres, profondeur max = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7883333333333333"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf50 = RandomForestClassifier(n_estimators=200, max_depth=100)\n",
    "clf50.fit(fit_train, y_train)\n",
    "clf50.score(fit_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4- Gradient Boost\n",
    "* Un méta-modèle de __gradient boosting__ devrait dépasser les RFs puisque les arbres ne sont plus appris indépendemment les uns des autres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7983333333333333"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gbc40 = GradientBoostingClassifier(n_estimators=200, max_depth=40)\n",
    "gbc40.fit(fit_train, y_train)\n",
    "gbc40.score(fit_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5- SVM linéaire avec la méthode OneVsRestClassifier\n",
    "\n",
    "Connu également sous le nom de one-vs-all  \n",
    "dans cette méta-classifieur, on crée un classifieur per classe.\n",
    "\n",
    "**Cette méta-méthode a besoin** (en paramètre) d'une méthode de classification à utiliser.\n",
    "\n",
    "Extrait de la Doc :\n",
    "\n",
    "For each classifier, the class is fitted against all the other classes. In addition to its computational efficiency (only n_classes classifiers are needed), one advantage of this approach is its interpretability. Since each class is represented by one and one classifier only, __it is possible to gain knowledge about the class by inspecting its corresponding classifier. This is the most commonly used strategy for multiclass classification and is a fair default choice__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8183333333333334"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "modele_one_vs_linear_SVC= OneVsRestClassifier(LinearSVC())\n",
    "modele_one_vs_linear_SVC.fit(fit_train, y_train)\n",
    "modele_one_vs_linear_SVC.score(fit_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__SVM se comporte souvent bien !__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6- Classifieur SVM linéaire avec la méthode OneVsRestClassifier\n",
    "\n",
    "Même chose mais on utilise la méthode SVM (pour la classification : SVC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "modele_one_vs_SVC = OneVsRestClassifier(SVC())\n",
    "modele_one_vs_SVC.fit(fit_train, y_train)\n",
    "modele_one_vs_SVC.score(fit_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Et en fonction des score, on choisit une des méthodes.**\n",
    "\n",
    "Pour simplifier, on choisit ici la régression logistique (**logit**) pour la suite.\n",
    "\n",
    "**MNB peut aussi faire l'affaire (voir ci-dessous).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4- Les n-grammes\n",
    "L'approche Bag of Word (matrices Tf, TfIdf)  ci-dessus ne tient pas compte ni du contexte, ni de l'ordre des mots. Chaque phrase est convertie en un sac de mots (ou bag of words). \n",
    "\n",
    "On va tenir compte de séquence plus ou moins longue à l'ide des __n-grams__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s'il faut télécharger des données\n",
    "# \n",
    "nltk_fait = True # A mettre dès le premier download\n",
    "if not nltk_fait :\n",
    "    import nltk\n",
    "    nltk.download('punkt')\n",
    "    nltk_fait == True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 : Un exemple de 3-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(None, None, 'Food'),\n",
       " (None, 'Food', 'was'),\n",
       " ('Food', 'was', 'really'),\n",
       " ('was', 'really', 'boring'),\n",
       " ('really', 'boring', '.'),\n",
       " ('boring', '.', None),\n",
       " ('.', None, None)]"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "generated_ngrams = ngrams(word_tokenize(X_train.iloc[0,0]), 3, pad_left=True, pad_right=True)\n",
    "list(generated_ngrams)[:7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2- Utilisation du bi-gram     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**On applique le principe de bi-gramme avec scikit-learn.**\n",
    "\n",
    "**On lémmatise,puis TfIdf sur les bi-grammes**\n",
    "\n",
    "Appliqué aux données de base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2250, 20237)"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "pipe2 = make_pipeline(CountVectorizer(ngram_range=(1, 2)), TfidfTransformer())\n",
    "pipe2.fit(X_train['Avis'])\n",
    "fit_train2 = pipe2.transform(X_train['Avis'])\n",
    "fit_train2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il y a plus de colonnes (normal !)\n",
    "\n",
    "**On vérifie** que les features ressemblent à des couples de mots (non traités / pas de lemmatisation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['5020', '5020 is', '510', '510 and', '510 maintains', '510 to',\n",
       "       '5lb', '5lb piece', '680', '70'], dtype=object)"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl = pipe2.steps[0]\n",
    "cl[1].get_feature_names_out()[100:110]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Gagné !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(750, 20237)"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_test2 = pipe2.transform(X_test['Avis'])\n",
    "fit_test2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1- Même aplication mais avec les données lemmatisées\n",
    "\n",
    "Appliqué aux données lemmatisées. (si Warning, lancez une 2e fois)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/venv-conda-3.9/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/alex/venv-conda-3.9/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['u'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2250, 22)\n"
     ]
    }
   ],
   "source": [
    "#count_vec_lemmatise = CountVectorizer(tokenizer=lemma, stop_words=\"english\", analyzer='word', \n",
    "#                            ngram_range=(1, 1), max_df=1.0, min_df=1, max_features=None)\n",
    "if True :\n",
    "    count_vec_lemmatise = CountVectorizer(tokenizer=lemma, stop_words=\"english\", analyzer='word', \n",
    "                                ngram_range=(1, 2), max_df=1.0, min_df=1, max_features=None)\n",
    "    pipe2_bis = make_pipeline(CountVectorizer(tokenizer=LemmaTokenizer(), stop_words=\"english\", analyzer='word', \n",
    "                                ngram_range=(1, 2), max_df=1.0, min_df=1, max_features=None), TfidfTransformer())\n",
    "    pipe2_bis.fit(X_train['Avis'])\n",
    "    fit_train2_bis = pipe2_bis.transform(X_train['Avis'])\n",
    "    print(fit_train2_bis.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True : \n",
    "    cl_bis = pipe2_bis.steps[0]\n",
    "    cl_bis[1].get_feature_names_out()[100:110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(750, 22)"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_test2_bis = pipe2_bis.transform(X_test['Avis'])\n",
    "fit_test2_bis.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 - Application de la méthode Logit aux bi-grammes (crées sur les données de base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-8 {color: black;background-color: white;}#sk-container-id-8 pre{padding: 0;}#sk-container-id-8 div.sk-toggleable {background-color: white;}#sk-container-id-8 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-8 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-8 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-8 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-8 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-8 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-8 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-8 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-8 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-8 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-8 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-8 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-8 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-8 div.sk-item {position: relative;z-index: 1;}#sk-container-id-8 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-8 div.sk-item::before, #sk-container-id-8 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-8 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-8 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-8 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-8 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-8 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-8 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-8 div.sk-label-container {text-align: center;}#sk-container-id-8 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-8 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-8\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" checked><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf2 = LogisticRegression(solver='lbfgs')\n",
    "clf2.fit(fit_train2, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Et on 'fit'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8493333333333334"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf2.score(fit_test2, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\"> Ce qui améliore   les résultats de façon significative.</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1- Application de la méthode Logit aux bi-grammes (crées sur les données lemmatisées)\n",
    "**La même chose avec les données lemmatisées :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression()\n"
     ]
    }
   ],
   "source": [
    "if True :\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    clf2_bis = LogisticRegression(solver='lbfgs')\n",
    "    print(clf2_bis.fit(fit_train2_bis, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Et on 'fit'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5333333333333333\n"
     ]
    }
   ],
   "source": [
    "if True :\n",
    "    print(clf2_bis.score(fit_test2_bis, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pas TOP !!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bisLemmaTokenizer(object):\n",
    "        def __init__(self):\n",
    "            self.wnl = WordNetLemmatizer()\n",
    "        def __call__(self, articles):\n",
    "            #return [lemma(t) for t in word_tokenize(articles) if t.lower() in words]\n",
    "\n",
    "            return [t for t in word_tokenize(articles) if t.lower() in words and \\\n",
    "                    t.lower() not in stop_words \\\n",
    "                    #and t.lower() not in word_tokenize(stop_words).encode() \\ # génère un pb de 'byte' ?!\n",
    "                    # cas des strs spécifiques non filtrés\n",
    "                    and t not in [\"''\", '--', '1.2', '1/2', '18th', '2-3', '20th', '4.00', '4.2', '``']\\\n",
    "                    and t.lower() not in string.punctuation and not t.isdigit()]\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**pas tellement Encourageant ?? !**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4- Cross-validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "On ne l'applique qu'aux données de bse (non lémmatisées)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-9 {color: black;background-color: white;}#sk-container-id-9 pre{padding: 0;}#sk-container-id-9 div.sk-toggleable {background-color: white;}#sk-container-id-9 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-9 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-9 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-9 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-9 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-9 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-9 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-9 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-9 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-9 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-9 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-9 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-9 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-9 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-9 div.sk-item {position: relative;z-index: 1;}#sk-container-id-9 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-9 div.sk-item::before, #sk-container-id-9 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-9 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-9 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-9 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-9 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-9 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-9 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-9 div.sk-label-container {text-align: center;}#sk-container-id-9 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-9 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-9\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegressionCV(cv=5)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" checked><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegressionCV</label><div class=\"sk-toggleable__content\"><pre>LogisticRegressionCV(cv=5)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegressionCV(cv=5)"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "clf2_ = LogisticRegressionCV(cv=5)\n",
    "clf2_.fit(fit_train2, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8613333333333333"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf2_.score(fit_test2, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">  Pas mal !</font>\n",
    "    \n",
    "__Rappel__ : Logit était un des meilleurs avec MNB.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 4.5-  Application de  MNB (sur les données de base)\n",
    "**D'abord MNB sur les données de de base !**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8506666666666667"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(fit_train2, y_train)\n",
    "mnb.score(fit_test2, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6- MNB avec Validation Croisée (XV)\n",
    "\n",
    "Pour rendre les résultats plus fiables, on doit faire XV en MNB.\n",
    "\n",
    "Pour cela, on applique \"k_folds MNB\" en partant du début du DB, transformé en 2-grams ...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**K-folds** pratiqué sur l'ensemble des données\n",
    "\n",
    "On a besoin de créer les folds soi-même pour MNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 25347)\n",
      "  (0, 23963)\t0.21817555202565733\n",
      "  (0, 23958)\t0.13646646001643659\n",
      "  (0, 23038)\t0.21817555202565733\n",
      "  (0, 23024)\t0.15086267181342922\n",
      "  (0, 22897)\t0.21817555202565733\n",
      "  (0, 22896)\t0.18179453499017778\n",
      "  (0, 22191)\t0.19998504350791754\n",
      "  (0, 22007)\t0.06983526375232646\n",
      "  (0, 21431)\t0.15086267181342922\n",
      "  (0, 21412)\t0.11373534790520554\n",
      "  (0, 21209)\t0.21817555202565733\n",
      "  (0, 20377)\t0.047500849751496446\n",
      "  (0, 18908)\t0.2075347866737308\n",
      "  (0, 18819)\t0.10268895845930259\n",
      "  (0, 16213)\t0.19998504350791754\n",
      "  (0, 16208)\t0.169053180331169\n",
      "  (0, 14096)\t0.21817555202565733\n",
      "  (0, 14033)\t0.12237002659602951\n",
      "  (0, 12915)\t0.17343724021803705\n",
      "  (0, 12888)\t0.1252488943815521\n",
      "  (0, 11195)\t0.17870351280406446\n",
      "  (0, 11084)\t0.06563245059819511\n",
      "  (0, 10933)\t0.17870351280406446\n",
      "  (0, 10761)\t0.06610745644244266\n",
      "  (0, 10446)\t0.11448165477794967\n",
      "  (0, 10369)\t0.21817555202565733\n",
      "  (0, 10317)\t0.16192793489944451\n",
      "  (0, 9706)\t0.19412900775576925\n",
      "  (0, 9692)\t0.12203139794376289\n",
      "  (0, 8644)\t0.21817555202565733\n",
      "  (0, 8638)\t0.12681612754354116\n",
      "  (0, 7968)\t0.169053180331169\n",
      "  (0, 7882)\t0.08556640209604605\n",
      "  (0, 4912)\t0.21817555202565733\n",
      "  (0, 3677)\t0.21817555202565733\n",
      "  (0, 3665)\t0.12237002659602951\n",
      "KFold(n_splits=20, random_state=0, shuffle=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_papers[[\"Avis\"]], df_papers['Polarite'])\n",
    "\n",
    "df_all=df_papers[['Avis','Polarite']]\n",
    "y_all=df_papers['Polarite']\n",
    "x_all=df_papers[\"Avis\"]\n",
    "\n",
    "pipe2_ = make_pipeline(CountVectorizer(ngram_range=(1, 2)),TfidfTransformer())\n",
    "pipe2_.fit(x_all)\n",
    "\n",
    "feat_train2_ = pipe2_.transform(x_all)\n",
    "print(feat_train2_.shape)\n",
    "print(feat_train2_[0])\n",
    "y_train2_=y_all\n",
    "\n",
    "Nb_folds=20\n",
    "k_fold = KFold(n_splits=Nb_folds , shuffle=True, random_state=0)\n",
    "k_fold.get_n_splits(df_all)\n",
    "print(k_fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6.1- Création du modèle -->  score poly K-Folds  MNB\n",
    "**Avec 10-folds, on atteint 0.84 (comme  Logit)**\n",
    "\n",
    "**Avec 20-folds, on atteint 0.853 (mieux  Logit), ...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRACE : Scores de chaque itération :  [0.8533333333333334, 0.8533333333333334, 0.8533333333333334, 0.8533333333333334, 0.8533333333333334, 0.8533333333333334, 0.8533333333333334, 0.8533333333333334, 0.8533333333333334, 0.8533333333333334, 0.8533333333333334, 0.8533333333333334, 0.8533333333333334, 0.8533333333333334, 0.8533333333333334, 0.8533333333333334, 0.8533333333333334, 0.8533333333333334, 0.8533333333333334, 0.8533333333333334]\n",
      "La moyenn :  0.8533333333333333\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "mnb2 = MultinomialNB()\n",
    "Scores=[]\n",
    "\n",
    "for i in range(Nb_folds) :\n",
    "    #print(i, end=' ')\n",
    "    res=next(k_fold.split(feat_train2_), None)\n",
    "    x_train_ = feat_train2_[res[0]]\n",
    "    x_test_ = feat_train2_[res[1]]\n",
    "    y_train_  = y_all.iloc[res[0]]\n",
    "    y_test_ = y_all.iloc[res[1]]\n",
    "    model__ = mnb2.fit(x_train_, y_train_)\n",
    "    predictions_ = mnb2.predict(x_test_)\n",
    "    Scores.append(model__.score(x_test_, y_test_))\n",
    "print('TRACE : Scores de chaque itération : ', Scores)\n",
    "print('La moyenn : ', np.mean(Scores))\n",
    "#mnb.fit(feat_train2, y_train)\n",
    "# mnb.score(feat_test2, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\"> __On s'est donc fait une idée du meilleur résultat jsq'à présent !__</font>\n",
    "\n",
    "**On avance .....**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.7- A la recherche d'améliorer les résultats....\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7.1  Réduction de dimension avec une SVD / ACP\n",
    "On choisit la méthode TruncatedSVD plutôt que l'ACP dont l'implantation ne supporte pas les features creux(sparses)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7.2- Application du SVD à __Tf__ d'abord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Avis'], dtype='object')\n",
      "2303                              Food was really boring.\n",
      "1381     God, and I can never get that 90 minutes back!  \n",
      "811                   #1 It Works - #2 It is Comfortable.\n",
      "1142    It is an hour and half waste of time, followin...\n",
      "1371    The plot was the same as pretty much every oth...\n",
      "                              ...                        \n",
      "2673                    What a great double cheeseburger!\n",
      "346             Reception is terrible and full of static.\n",
      "2697                                     I was mortified.\n",
      "1129    However, this didn't make up for the fact that...\n",
      "2538                                  Worst martini ever!\n",
      "Name: Avis, Length: 2250, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(X_train.keys())\n",
    "print(X_train['Avis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2250, 300)"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "pipe_svd = make_pipeline(CountVectorizer(), TruncatedSVD(n_components=300))\n",
    "pipe_svd.fit(X_train['Avis'])\n",
    "fit_train_svd = pipe_svd.transform(X_train['Avis'])\n",
    "fit_train_svd.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7.3-  Application du  RF à ce résultat SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-10 {color: black;background-color: white;}#sk-container-id-10 pre{padding: 0;}#sk-container-id-10 div.sk-toggleable {background-color: white;}#sk-container-id-10 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-10 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-10 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-10 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-10 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-10 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-10 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-10 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-10 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-10 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-10 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-10 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-10 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-10 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-10 div.sk-item {position: relative;z-index: 1;}#sk-container-id-10 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-10 div.sk-item::before, #sk-container-id-10 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-10 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-10 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-10 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-10 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-10 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-10 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-10 div.sk-label-container {text-align: center;}#sk-container-id-10 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-10 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-10\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(n_estimators=50)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" checked><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(n_estimators=50)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(n_estimators=50)"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_svd = RandomForestClassifier(n_estimators=50)\n",
    "clf_svd.fit(fit_train_svd, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7053333333333334"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_test_svd = pipe_svd.transform(X_test['Avis'])\n",
    "clf_svd.score(fit_test_svd, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7866666666666666"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_svd = LogisticRegression(solver='lbfgs')\n",
    "lr_svd.fit(fit_train_svd, y_train)\n",
    "lr_svd.score(fit_test_svd, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 4.8-Comparaison des méthodes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Repartons de TF-IDF puis SVD puis Logit pour comparer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_svd_tfidf = make_pipeline(CountVectorizer(), \n",
    "                     TfidfTransformer(),\n",
    "                     TruncatedSVD(n_components=300))\n",
    "pipe_svd_tfidf.fit(X_train['Avis'])\n",
    "fit_train_svd_tfidf = pipe_svd_tfidf.transform(X_train['Avis'])\n",
    "\n",
    "clf_svd_tfidf = LogisticRegression(solver='lbfgs')\n",
    "clf_svd_tfidf.fit(fit_train_svd_tfidf, y_train)\n",
    "\n",
    "fit_test_svd_tfidf = pipe_svd_tfidf.transform(X_test['Avis'])\n",
    "clf_svd_tfidf.score(fit_test_svd_tfidf, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\"> C'est (un peu) mieux mais cela reste moins bien que le tf-idf sans réduction de dimensions. </font>\n",
    "\n",
    "**Cela veut dire qu'il faut garder davantage de dimensions.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5- word2vec\n",
    "word2vec est une projection (comme ACP/SVD) en ce sens qu'il réduit les dimensions. \n",
    "\n",
    "Une relecture d'ACP et Auto Encoders pour comprendre le lien entre ACP, ACP non linéaire, réseaux de neurones et compression nous fera du bien !\n",
    "\n",
    "__word2vec__ est plus d'une ACP non linéaire car il prend en compte le contexte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OK  FAIT : !/Users/alexandresaidi/opt/anaconda3/bin/pip3 install gensim\n",
    "# OK  FAIT : !/Users/alexandresaidi/opt/anaconda3/bin/pip3 install p Levenshtein\n",
    "if False :\n",
    "    !pip install gensim\n",
    "    !pip install Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['his',\n",
       " 'on',\n",
       " 'screen',\n",
       " 'presence',\n",
       " 'shined',\n",
       " 'thought',\n",
       " 'even',\n",
       " 'though',\n",
       " 'there',\n",
       " 'were',\n",
       " 'other',\n",
       " 'senior',\n",
       " 'actors',\n",
       " 'on',\n",
       " 'the',\n",
       " 'screen',\n",
       " 'with',\n",
       " 'him']"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.utils import tokenize\n",
    "Avis = [list(tokenize(s, deacc=True, lower=True)) for s in X_train['Avis']]\n",
    "Avis[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**N.B. : Les paramètres d'apprentissage du modèle Word2Vec ne sont pas toujours décrit de façon explicite.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2250"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "#model = word2vec.Word2Vec(Avis, size=300, window=20, min_count=2, workers=1, iter=100)\n",
    "model = word2vec.Word2Vec(Avis,  window=20, min_count=2, workers=1)\n",
    "model.corpus_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 HALT : Save the model !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('trained_word2vec.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Vecteur associé aux mots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vecteur associé au mot after**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.12766968,  0.41114926, -0.01115404,  0.05169382,  0.16325916,\n",
       "       -0.74640715,  0.2699146 ,  0.89105666, -0.40607375, -0.32883108,\n",
       "       -0.1453936 , -0.39305004,  0.04914179,  0.26536164,  0.13936654,\n",
       "       -0.30557168,  0.36342728, -0.45275763,  0.08435892, -0.91476095,\n",
       "        0.4810084 ,  0.16188471,  0.30201578, -0.34207466, -0.2938233 ,\n",
       "       -0.03079732, -0.3347006 , -0.17213261, -0.25524208,  0.04822668,\n",
       "        0.47608572, -0.06135466,  0.1910234 , -0.34661013, -0.02943614,\n",
       "        0.5755988 ,  0.18558192, -0.23952731, -0.2783688 , -0.5842022 ,\n",
       "        0.1344876 , -0.30491477, -0.15895472,  0.21584024,  0.35471866,\n",
       "       -0.16082133, -0.35555416, -0.13018811,  0.21603845,  0.179608  ,\n",
       "        0.14026982, -0.29226223, -0.0870752 , -0.0263117 , -0.08850314,\n",
       "        0.10073767,  0.2766956 , -0.14003183, -0.39824146,  0.13667043,\n",
       "        0.11864848, -0.00782809,  0.1602534 , -0.00647726, -0.37263742,\n",
       "        0.3935823 ,  0.3362208 ,  0.34554753, -0.692624  ,  0.5939052 ,\n",
       "       -0.2793512 ,  0.23836079,  0.4790181 ,  0.02763588,  0.43956238,\n",
       "        0.07918125, -0.15691797,  0.23367518, -0.39154252, -0.02579045,\n",
       "       -0.38118294, -0.02981028, -0.34157002,  0.6852267 , -0.23661949,\n",
       "        0.06189811,  0.366165  ,  0.48069197,  0.5687485 ,  0.07860505,\n",
       "        0.6700779 ,  0.32005134,  0.01439146,  0.04205808,  0.79801255,\n",
       "        0.34147644,  0.15787946, -0.33124325,  0.12745   , -0.11372256],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = model.wv # .vocab\n",
    "# w2v.vocab[\"word\"]\n",
    "#list(vocab)#[:5]\n",
    "vocab['after']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Les dix premières coordonnées du vecteur associé au mot after.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100,),\n",
       " array([-0.12766968,  0.41114926, -0.01115404,  0.05169382,  0.16325916,\n",
       "        -0.74640715,  0.2699146 ,  0.89105666, -0.40607375, -0.32883108],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['after'].shape, model.wv['after'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Et si le mot est inconnu**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Key 'toto' not present\"\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(model.wv['toto'])\n",
    "except KeyError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Similarité entre les mots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3 mots les plus proches de \"movie\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('of', 0.9998689889907837),\n",
       " ('and', 0.9998657703399658),\n",
       " ('the', 0.9998517036437988)]"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['movie'], topn = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3 mots les plus proches de \"after\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('and', 0.9998399019241333),\n",
       " ('was', 0.9998385906219482),\n",
       " ('the', 0.9998370409011841)]"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['after'], topn = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3 mots les plus proches de \"young\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('call', 0.9877576231956482),\n",
       " ('super', 0.9877095818519592),\n",
       " ('films', 0.9876782298088074)]"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['young'], topn = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6- Vers Doc2Vect (manuel & expérimental)\n",
    "\n",
    "**Pour chaque phrase, on fait la somme des vecteurs associés aux mots qui la composent ou pas si le mot n'est pas dans le vocabulaire.**\n",
    "\n",
    "* Proche de __doc2vect__\n",
    "* Il y a probablement des fonctions déjà prêtes à l'emploi mais la documentation de gensim n'était pas assez explicite \n",
    "\n",
    "Réfs : \n",
    "\n",
    "Efficient Estimation of Word Representations in Vector Space     \n",
    "\n",
    "Distributed Representations of Words and Phrases and their Compositionality)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NB :  :  il existe des fonctions équivalentes !!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2250, 100)"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_vect(word, model):\n",
    "    try:\n",
    "        return model.wv[word]\n",
    "    except KeyError:\n",
    "        return np.zeros((model.vector_size,))\n",
    "# Somme\n",
    "def sum_vectors(phrase, model):\n",
    "    return sum(get_vect(w, model) for w in phrase)\n",
    "\n",
    "def word2vec_features(X, model):\n",
    "    feats = np.vstack([sum_vectors(p, model) for p in X])\n",
    "    return feats\n",
    "\n",
    "wv_train_feat = word2vec_features(X_train[\"Avis\"], model)\n",
    "wv_train_feat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Logit sur word2vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-11 {color: black;background-color: white;}#sk-container-id-11 pre{padding: 0;}#sk-container-id-11 div.sk-toggleable {background-color: white;}#sk-container-id-11 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-11 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-11 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-11 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-11 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-11 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-11 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-11 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-11 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-11 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-11 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-11 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-11 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-11 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-11 div.sk-item {position: relative;z-index: 1;}#sk-container-id-11 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-11 div.sk-item::before, #sk-container-id-11 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-11 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-11 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-11 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-11 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-11 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-11 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-11 div.sk-label-container {text-align: center;}#sk-container-id-11 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-11 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-11\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=1000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" checked><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=1000)"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clfwv = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "clfwv.fit(wv_train_feat, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -7.03691576,  24.60918842,  -0.75375295, ..., -19.34270166,\n",
       "          7.85724418,  -7.19071147],\n",
       "       [-10.02819068,  35.42520335,  -1.1167499 , ..., -27.71937919,\n",
       "         11.1212775 , -10.25891086],\n",
       "       [ -7.7444046 ,  27.28239491,  -0.9141875 , ..., -21.47527908,\n",
       "          8.68236897,  -8.06971514],\n",
       "       ...,\n",
       "       [ -2.9423628 ,  10.31470088,  -0.27300781, ...,  -8.13426027,\n",
       "          3.28717501,  -3.02347554],\n",
       "       [ -3.47238301,  12.30123544,  -0.43551564, ...,  -9.6420259 ,\n",
       "          3.84918905,  -3.60390083],\n",
       "       [ -1.67162384,   5.74246043,  -0.17529328, ...,  -4.54993329,\n",
       "          1.8021079 ,  -1.6969712 ]])"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv_test_feat = word2vec_features(X_test[\"Avis\"], model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5533333333333333"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clfwv.score(wv_test_feat, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NB**\n",
    "\n",
    "La performance est bien moindre et encore  moindre que la performance obtenue avec l'ACP. \n",
    "Il faudrait 'tuner' les hyperparamètres de l'apprentissage ou réutiliser un model appris sur un corpus similaire aux données initiales mais bien plus grand. \n",
    "\n",
    "On peut constater que la fonction de similarités ne retourne pas des résultat très intéressants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'ability',\n",
       " 'able',\n",
       " 'about',\n",
       " 'above',\n",
       " 'absolutely',\n",
       " 'abysmal',\n",
       " 'access',\n",
       " 'acted',\n",
       " 'acting']"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.index_to_key[:10]\n",
    "# words = list(sorted(model.wv))\n",
    "words=sorted(model.wv.index_to_key)\n",
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1194682/3882633673.py:7: FutureWarning: In a future version of pandas all arguments of DataFrame.pivot will be keyword-only.\n",
      "  pandas.DataFrame(rows).pivot(\"w1\", \"w2\", \"d\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>w2</th>\n",
       "      <th>about</th>\n",
       "      <th>above</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>after</th>\n",
       "      <th>before</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w1</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>about</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.993614</td>\n",
       "      <td>0.999089</td>\n",
       "      <td>0.999712</td>\n",
       "      <td>0.999027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>above</th>\n",
       "      <td>0.993614</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.993084</td>\n",
       "      <td>0.993799</td>\n",
       "      <td>0.993214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>absolutely</th>\n",
       "      <td>0.999089</td>\n",
       "      <td>0.993084</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999136</td>\n",
       "      <td>0.998575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>after</th>\n",
       "      <td>0.999712</td>\n",
       "      <td>0.993799</td>\n",
       "      <td>0.999136</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>before</th>\n",
       "      <td>0.999027</td>\n",
       "      <td>0.993214</td>\n",
       "      <td>0.998575</td>\n",
       "      <td>0.998972</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "w2             about     above  absolutely     after    before\n",
       "w1                                                            \n",
       "about       1.000000  0.993614    0.999089  0.999712  0.999027\n",
       "above       0.993614  1.000000    0.993084  0.993799  0.993214\n",
       "absolutely  0.999089  0.993084    1.000000  0.999136  0.998575\n",
       "after       0.999712  0.993799    0.999136  1.000000  0.998972\n",
       "before      0.999027  0.993214    0.998575  0.998972  1.000000"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset = ['after', 'before', words[3], words[4], words[5]]\n",
    "rows = []\n",
    "for w in subset:\n",
    "    for ww in subset:\n",
    "        rows.append(dict(w1=w, w2=ww, d=model.wv.similarity(w, ww)))\n",
    "import pandas\n",
    "pandas.DataFrame(rows).pivot(\"w1\", \"w2\", \"d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# That's ALL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercice \n",
    "\n",
    "Choisir quelques phrases en anglais (une centaine) et essayer de faire quelques un de ces manipulations.\n",
    "\n",
    "Essayer d'utilliser les résultats Word2Vect de Google (attention BD un peu grande), appliquer word2vect et comparer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-3.9",
   "language": "python",
   "name": "anaconda-3.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc-showcode": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
